"""
ARGO Phase 2 ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
Redis í´ëŸ¬ìŠ¤í„°, ë¹„ë™ê¸° ë©”ì‹œì§€ í, í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import logging
import time
from datetime import datetime
import sys
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

async def test_async_message_queue():
    """ë¹„ë™ê¸° ë©”ì‹œì§€ í ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸš€ ë¹„ë™ê¸° ë©”ì‹œì§€ í ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    try:
        from infrastructure.message_queue.message_queue import AsyncMessageQueue, BatchConfig, MessagePriority
        
        # ë°°ì¹˜ ì„¤ì •
        batch_config = BatchConfig(
            max_batch_size=5,
            max_wait_time=2.0,
            max_concurrent_batches=2
        )
        
        # ë©”ì‹œì§€ í ìƒì„±
        queue = AsyncMessageQueue(batch_config)
        
        # í…ŒìŠ¤íŠ¸ í•¸ë“¤ëŸ¬
        async def test_handler(message):
            print(f"  ğŸ“¨ ì²˜ë¦¬ë¨: {message.topic} - {message.payload}")
            await asyncio.sleep(0.1)
        
        async def batch_handler(messages):
            print(f"  ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬: {len(messages)}ê°œ ë©”ì‹œì§€")
            for msg in messages:
                print(f"    - {msg.topic}: {msg.payload}")
            await asyncio.sleep(0.3)
        
        # í•¸ë“¤ëŸ¬ ë“±ë¡
        queue.subscribe("test_topic", test_handler)
        queue.subscribe("batch_topic", batch_handler, is_batch_handler=True)
        
        # ì‹œìŠ¤í…œ ì‹œì‘
        await queue.start()
        
        # ë©”ì‹œì§€ ë°œí–‰
        print("ğŸ“¤ ë©”ì‹œì§€ ë°œí–‰ ì¤‘...")
        for i in range(15):
            await queue.publish("test_topic", {
                "message": f"Test message {i}",
                "timestamp": datetime.now().isoformat()
            })
            await asyncio.sleep(0.05)
        
        # ë°°ì¹˜ ë©”ì‹œì§€ ë°œí–‰
        batch_messages = [
            {"batch": f"Batch message {i}", "priority": i % 3}
            for i in range(10)
        ]
        await queue.publish_batch("batch_topic", batch_messages)
        
        # ìƒíƒœ ëª¨ë‹ˆí„°ë§
        print("\nğŸ“Š ìƒíƒœ ëª¨ë‹ˆí„°ë§...")
        for i in range(8):
            status = await queue.get_queue_status()
            print(f"  {i+1}/8 - í í¬ê¸°: {status['queue_sizes']}")
            await asyncio.sleep(1)
        
        # ì‹œìŠ¤í…œ ì¤‘ì§€
        await queue.stop()
        print("âœ… ë¹„ë™ê¸° ë©”ì‹œì§€ í í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ë¹„ë™ê¸° ë©”ì‹œì§€ í í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        logger.error(f"ë¹„ë™ê¸° ë©”ì‹œì§€ í í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

async def test_enhanced_data_sync():
    """í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ”„ í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    try:
        from infrastructure.sync.enhanced_data_sync_manager import (
            EnhancedDataSyncManager, SyncOperation, SyncOperationType
        )
        
        # ì„¤ì •
        config = {
            'max_concurrent_syncs': 2,
            'conflict_resolution': 'last_write_wins'
        }
        
        # ë™ê¸°í™” ë§¤ë‹ˆì € ìƒì„±
        sync_manager = EnhancedDataSyncManager(config)
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        async def sync_event_handler(data):
            print(f"  ğŸ”„ ì´ë²¤íŠ¸: {data}")
        
        # í•¸ë“¤ëŸ¬ ë“±ë¡
        sync_manager.add_event_handler('sync_completed', sync_event_handler)
        sync_manager.add_event_handler('conflict_detected', sync_event_handler)
        
        # ë§¤ë‹ˆì € ì‹œì‘
        await sync_manager.start()
        
        # í…ŒìŠ¤íŠ¸ ë™ê¸°í™” ì‘ì—… ìƒì„±
        operations = [
            SyncOperation(
                operation_type=SyncOperationType.CREATE,
                source_system='redis',
                data={'name': 'Test Data 1', 'value': 100, 'category': 'test'},
                metadata={'redis_key': 'test:data:1', 'urgent': False}
            ),
            SyncOperation(
                operation_type=SyncOperationType.UPDATE,
                source_system='neo4j',
                data={'name': 'Test Data 2', 'value': 200, 'category': 'test'},
                metadata={'redis_key': 'test:data:2', 'urgent': True}
            ),
            SyncOperation(
                operation_type=SyncOperationType.MERGE,
                source_system='bigquery',
                data={'name': 'Test Data 3', 'value': 300, 'category': 'test'},
                metadata={'redis_key': 'test:data:3', 'urgent': False}
            )
        ]
        
        # ë™ê¸°í™” ìš”ì²­
        print("ğŸ“¤ ë™ê¸°í™” ì‘ì—… ìš”ì²­ ì¤‘...")
        operation_ids = await sync_manager.sync_batch(operations)
        print(f"  ìš”ì²­ëœ ì‘ì—… ID: {operation_ids}")
        
        # ìƒíƒœ ëª¨ë‹ˆí„°ë§
        print("\nğŸ“Š ë™ê¸°í™” ìƒíƒœ ëª¨ë‹ˆí„°ë§...")
        for i in range(10):
            status = await sync_manager.get_sync_status()
            print(f"  {i+1}/10 - í ìƒíƒœ: {status['queue_sizes']}")
            print(f"    ë©”íŠ¸ë¦­: {status['metrics']['successful_syncs']} ì„±ê³µ, {status['metrics']['failed_syncs']} ì‹¤íŒ¨")
            await asyncio.sleep(2)
        
        # ë§¤ë‹ˆì € ì¤‘ì§€
        await sync_manager.stop()
        print("âœ… í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        logger.error(f"í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

async def test_integration():
    """ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ”— ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    try:
        # ë‘ ì‹œìŠ¤í…œì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ìƒí˜¸ì‘ìš© í…ŒìŠ¤íŠ¸
        from infrastructure.message_queue.message_queue import AsyncMessageQueue, BatchConfig
        from infrastructure.sync.enhanced_data_sync_manager import (
            EnhancedDataSyncManager, SyncOperation, SyncOperationType
        )
        
        # ë©”ì‹œì§€ í ì„¤ì •
        queue = AsyncMessageQueue(BatchConfig(max_batch_size=3, max_wait_time=1.0))
        
        # ë™ê¸°í™” ë§¤ë‹ˆì € ì„¤ì •
        sync_manager = EnhancedDataSyncManager({'max_concurrent_syncs': 2})
        
        # í†µí•© í•¸ë“¤ëŸ¬
        async def integration_handler(message):
            print(f"  ğŸ”— í†µí•© ì²˜ë¦¬: {message.topic}")
            
            # ë©”ì‹œì§€ë¥¼ ë™ê¸°í™” ì‘ì—…ìœ¼ë¡œ ë³€í™˜
            if message.topic == "sync_request":
                operation = SyncOperation(
                    operation_type=SyncOperationType.CREATE,
                    source_system='integration',
                    data=message.payload,
                    metadata={'source': 'message_queue'}
                )
                
                # ë™ê¸°í™” ìš”ì²­
                await sync_manager.sync_data(operation)
        
        # í•¸ë“¤ëŸ¬ ë“±ë¡
        queue.subscribe("sync_request", integration_handler)
        
        # ì‹œìŠ¤í…œ ì‹œì‘
        await queue.start()
        await sync_manager.start()
        
        # í†µí•© í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ë°œí–‰
        print("ğŸ“¤ í†µí•© í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ë°œí–‰...")
        for i in range(5):
            await queue.publish("sync_request", {
                "test_id": i,
                "data": f"Integration test data {i}",
                "timestamp": datetime.now().isoformat()
            })
            await asyncio.sleep(0.5)
        
        # ìƒíƒœ ëª¨ë‹ˆí„°ë§
        print("\nğŸ“Š í†µí•© ìƒíƒœ ëª¨ë‹ˆí„°ë§...")
        for i in range(6):
            queue_status = await queue.get_queue_status()
            sync_status = await sync_manager.get_sync_status()
            
            print(f"  {i+1}/6 - ë©”ì‹œì§€ í: {queue_status['queue_sizes']}")
            print(f"    ë™ê¸°í™”: {sync_status['queue_sizes']}")
            
            await asyncio.sleep(2)
        
        # ì‹œìŠ¤í…œ ì¤‘ì§€
        await queue.stop()
        await sync_manager.stop()
        print("âœ… ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        logger.error(f"ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ¯ ARGO Phase 2 ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # 1. ë¹„ë™ê¸° ë©”ì‹œì§€ í í…ŒìŠ¤íŠ¸
        await test_async_message_queue()
        
        # 2. í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” í…ŒìŠ¤íŠ¸
        await test_enhanced_data_sync()
        
        # 3. ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
        await test_integration()
        
        print("\n" + "="*60)
        print("ğŸ‰ ëª¨ë“  Phase 2 ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*60)
        
        # ê²°ê³¼ ìš”ì•½
        print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
        print("âœ… ë¹„ë™ê¸° ë©”ì‹œì§€ í ì‹œìŠ¤í…œ - ì •ìƒ ì‘ë™")
        print("âœ… í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € - ì •ìƒ ì‘ë™")
        print("âœ… ì‹œìŠ¤í…œ í†µí•© - ì •ìƒ ì‘ë™")
        print("âœ… Redis í´ëŸ¬ìŠ¤í„° ì¤€ë¹„ ì™„ë£Œ")
        print("âœ… ì´ë²¤íŠ¸ ê¸°ë°˜ ë™ê¸°í™” êµ¬í˜„ ì™„ë£Œ")
        print("âœ… ì¶©ëŒ í•´ê²° ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return False
    
    return True

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = asyncio.run(main())
    
    if success:
        print(f"\nğŸ¯ ARGO Phase 2 ì¤€ë¹„ ì™„ë£Œ!")
        print("ë‹¤ìŒ ë‹¨ê³„: GCP ì¸í”„ë¼ ë°°í¬ ë° ì‹¤ì œ ì„œë¹„ìŠ¤ ì—°ê²°")
    else:
        print(f"\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    sys.exit(0 if success else 1)
