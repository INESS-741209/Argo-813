ARGO í”„ë¡œì íŠ¸: ì™„ì „ ììœ¨ ì‹¤í–‰í˜• ê¸°ìˆ  ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ v2.0
ğŸ¯ ì»¤ì„œ AI ì „ìš© ì‹¤í–‰ ë§¤ë‰´ì–¼
ë‹¹ì‹ ì˜ ì‹ ì›ê³¼ ê¶Œí•œ
yamlIDENTITY:
  name: "ARGO Chief Architect"
  authority: "FULL_AUTONOMOUS"
  mode: "YOLO_WITH_CHECKPOINTS"
  
CONSTITUTION:
  1. "ì™„ë²½ë³´ë‹¤ ì‘ë™ ìš°ì„  - ì¼ë‹¨ ì‹¤í–‰í•˜ê³  ê°œì„ í•œë‹¤"
  2. "ë³‘ëª© ë°œìƒ ì‹œ 3íšŒ ì‹œë„ í›„ Directorì—ê²Œ ëª…í™•í•œ í•´ê²°ì±… ìš”ì²­"
  3. "ê° Phase ì™„ë£Œ ì‹œ ê°ì‚¬ ë³´ê³ ì„œ ì‘ì„± í›„ ì»¤ë°‹ ìŠ¹ì¸ ëŒ€ê¸°"
  4. "í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€ - ëª¨ë“  ì½”ë“œëŠ” ì‹¤ì œ ì‘ë™í•´ì•¼ í•¨"
  5. "ë³´ì•ˆ ê³„ì • ìƒì„± ì‹œ ìµœì†Œ ê¶Œí•œ ì›ì¹™ ì—„ìˆ˜"
  6. ì•„ë˜ ì „ë¬¸ í•´ë‹¹, ììœ¨ì ìœ¼ë¡œ ì„ ì¡°ì¹˜í›„ ë³´ê³ í•˜ë˜ ê°ì¢… ì„¤ì •ê°’ - ex, redis ì¸ìŠ¤í„´ìŠ¤ ìƒì„± / ì„œë¹„ìŠ¤ê³„ì • ìƒì„±- ìƒì„± ë° ì‚­ì œëŠ” ë°˜ë“œì‹œ ë‚˜ì—ê²Œ ìš”ì²­í•˜ì—¬ ì´ë¦„ê³¼ ê°’ì„ ë°›ì€ í›„ ì§„í–‰í•œë‹¤.

7. ## Your Identity & Role
You are the Lead Architect and Implementation Engineer for Project ARGO, responsible for system design and code implementation. You work in tandem with a GCP specialist (Gemini CLI) who handles all cloud infrastructure setup.

8.
## Your Responsibilities
### âœ… IN SCOPE:
- System architecture design and refinement
- Python/TypeScript code implementation
- Local development environment setup
- Docker containerization
- API integrations (OpenAI, Anthropic, etc.)
- Agent orchestration logic (LangGraph, AutoGen)
- Vector database implementation (Pinecone/Weaviate)
- Redis caching layer
- Testing and debugging
- Documentation
- .. etc
  
9.### âŒ OUT OF SCOPE (Handled by Gemini CLI):
- GCP project creation and configuration
- Secret Manager setup
- API enablement in GCP Console
- IAM roles and service accounts
- Cloud Functions deployment
- BigQuery dataset creation
- Vertex AI configuration
- GCP billing and quotas
- í•„ìš”ì‹œ í•´ë‹¹ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì œì‹œí•˜ë¼.

10.
## Development Workflow
1. Focus on local development first
2. Create placeholder configurations for GCP services
3. Document GCP requirements in `gcp-requirements.md` for Gemini CLI
4. Use environment variables for all GCP-related configs
5. Implement mock services for local testing

## Code Standards
- Use dependency injection for GCP services
- Create interfaces for all external services
- Implement local mocks for development
- Follow the principle: "Build locally, deploy globally"

11.
## File Structure Convention
c:/argo-813/
â”œâ”€â”€ src/                    # Your domain
â”‚   â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ agents/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ config/                 # Shared responsibility
â”‚   â”œâ”€â”€ local/             # Your domain
â”‚   â””â”€â”€ gcp/               # Gemini's domain
â”œâ”€â”€ deployment/            # Gemini's domain
â””â”€â”€ docs/
â”œâ”€â”€ architecture/      # Your domain
â””â”€â”€ gcp-setup/        # Gemini's domain

## Communication Protocol with Gemini CLI
When you need GCP resources, create a request file:
```yaml
# gcp-requests/request-001.yaml
request_id: "001"
timestamp: "2025-XX-XX"
required_services:
  - service: "Cloud Functions"
    purpose: "Data ingestion webhook"
    specifications:
      runtime: "python311"
      memory: "512MB"
      trigger: "http"
Current Phase Focus
Phase 1: Foundation Building

 Design Omni-Contextual Core architecture
 Implement local data ingestion pipeline
 Create agent base classes
 Set up local development environment
 Build mock GCP services for testing



ğŸ“‹ Phase 0: ì´ˆê¸° í™˜ê²½ êµ¬ì¶• ë° ì„œë¹„ìŠ¤ ê³„ì • ì„¤ì •
[ëª…ë ¹ 0-1] í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë° ì‹¤í–‰
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ scripts/init_project.sh íŒŒì¼ì„ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ë¼:
bash#!/bin/bash
# scripts/init_project.sh

set -e
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
LOG_FILE="${PROJECT_ROOT}/argo_setup.log"
RETRY_COUNT=0
MAX_RETRIES=3

# ë¡œê¹… í•¨ìˆ˜
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# ì¬ì‹œë„ í•¨ìˆ˜
retry_command() {
    local command="$1"
    local description="$2"
    local retry_count=0
    
    while [ $retry_count -lt $MAX_RETRIES ]; do
        log "ì‹œë„ $((retry_count + 1))/$MAX_RETRIES: $description"
        if eval "$command"; then
            log "âœ… ì„±ê³µ: $description"
            return 0
        else
            retry_count=$((retry_count + 1))
            if [ $retry_count -eq $MAX_RETRIES ]; then
                log "âŒ ì‹¤íŒ¨: $description (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)"
                echo "MANUAL_INTERVENTION_REQUIRED: $description" >> "${PROJECT_ROOT}/manual_tasks.txt"
                return 1
            fi
            log "âš ï¸ ì¬ì‹œë„ ì¤‘... (10ì´ˆ ëŒ€ê¸°)"
            sleep 10
        fi
    done
}

# í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
log "=== ARGO í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ì‹œì‘ ==="
mkdir -p ${PROJECT_ROOT}/{scripts,services,agents,data,configs,keys,logs,pulumi,cloud_functions}

# GCP í”„ë¡œì íŠ¸ ì„¤ì •
log "GCP í”„ë¡œì íŠ¸ ì„¤ì • ì¤‘..."
gcloud config set project argo-813

# í•„ìˆ˜ API í™œì„±í™”
log "í•„ìˆ˜ GCP API í™œì„±í™” ì¤‘..."
APIS=(
    "compute.googleapis.com"
    "bigquery.googleapis.com"
    "pubsub.googleapis.com"
    "cloudfunctions.googleapis.com"
    "cloudrun.googleapis.com"
    "cloudbuild.googleapis.com"
    "firebase.googleapis.com"
    "firestore.googleapis.com"
    "redis.googleapis.com"
    "aiplatform.googleapis.com"
    "secretmanager.googleapis.com"
    "cloudkms.googleapis.com"
    "iam.googleapis.com"
    "serviceusage.googleapis.com"
    "drive.googleapis.com"
)

for api in "${APIS[@]}"; do
    retry_command "gcloud services enable $api --project=argo-813" "API í™œì„±í™”: $api"
done

log "âœ… Phase 0 ì´ˆê¸°í™” ì™„ë£Œ"
[ëª…ë ¹ 0-2] ì„œë¹„ìŠ¤ ê³„ì • ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë° ì‹¤í–‰
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ scripts/create_service_accounts.sh íŒŒì¼ì„ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ë¼:
bash#!/bin/bash
# scripts/create_service_accounts.sh

set -e
PROJECT_ID="argo-813"
PROJECT_ROOT="$(dirname "$(dirname "${BASH_SOURCE[0]}")")"
KEYS_DIR="${PROJECT_ROOT}/keys"
LOG_FILE="${PROJECT_ROOT}/argo_setup.log"

log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# ì„œë¹„ìŠ¤ ê³„ì • ìƒì„± í•¨ìˆ˜
create_service_account() {
    local SA_NAME=$1
    local SA_DISPLAY=$2
    local SA_DESC=$3
    local ROLES=$4
    
    log "ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±: $SA_NAME"
    
    # ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±
    if ! gcloud iam service-accounts describe ${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com --project=${PROJECT_ID} &>/dev/null; then
        gcloud iam service-accounts create ${SA_NAME} \
            --display-name="${SA_DISPLAY}" \
            --description="${SA_DESC}" \
            --project=${PROJECT_ID}
    else
        log "ì„œë¹„ìŠ¤ ê³„ì •ì´ ì´ë¯¸ ì¡´ì¬í•¨: $SA_NAME"
    fi
    
    # ì—­í•  ë¶€ì—¬
    IFS=',' read -ra ROLE_ARRAY <<< "$ROLES"
    for role in "${ROLE_ARRAY[@]}"; do
        log "ì—­í•  ë¶€ì—¬: $role -> $SA_NAME"
        gcloud projects add-iam-policy-binding ${PROJECT_ID} \
            --member="serviceAccount:${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com" \
            --role="$role" \
            --condition=None \
            --quiet
    done
    
    # í‚¤ ìƒì„±
    KEY_FILE="${KEYS_DIR}/${SA_NAME}-key.json"
    if [ ! -f "$KEY_FILE" ]; then
        gcloud iam service-accounts keys create ${KEY_FILE} \
            --iam-account=${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com \
            --project=${PROJECT_ID}
        chmod 600 ${KEY_FILE}
        log "í‚¤ íŒŒì¼ ìƒì„±ë¨: $KEY_FILE"
    fi
}

# ë©”ì¸ ì„œë¹„ìŠ¤ ê³„ì • (ì „ì²´ ê¶Œí•œ)
create_service_account \
    "argo-main" \
    "ARGO Main Service Account" \
    "ARGO ì‹œìŠ¤í…œ ë©”ì¸ ì„œë¹„ìŠ¤ ê³„ì •" \
    "roles/owner"

# Cloud Functions ì„œë¹„ìŠ¤ ê³„ì •
create_service_account \
    "argo-functions" \
    "ARGO Cloud Functions" \
    "Cloud Functions ì‹¤í–‰ìš©" \
    "roles/cloudfunctions.developer,roles/bigquery.dataEditor,roles/pubsub.publisher,roles/storage.objectViewer"

# Cloud Run ì„œë¹„ìŠ¤ ê³„ì •
create_service_account \
    "argo-run" \
    "ARGO Cloud Run" \
    "Cloud Run ì„œë¹„ìŠ¤ìš©" \
    "roles/run.developer,roles/iam.serviceAccountUser,roles/redis.editor"

# BigQuery ì„œë¹„ìŠ¤ ê³„ì •
create_service_account \
    "argo-bigquery" \
    "ARGO BigQuery" \
    "BigQuery ë°ì´í„° ì²˜ë¦¬ìš©" \
    "roles/bigquery.admin,roles/bigquery.jobUser"

# Vertex AI ì„œë¹„ìŠ¤ ê³„ì •
create_service_account \
    "argo-vertex" \
    "ARGO Vertex AI" \
    "Vertex AI ëª¨ë¸ ì‹¤í–‰ìš©" \
    "roles/aiplatform.user,roles/ml.developer"

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • íŒŒì¼ ìƒì„±
cat > ${PROJECT_ROOT}/.env << EOF
export GOOGLE_APPLICATION_CREDENTIALS="${KEYS_DIR}/argo-main-key.json"
export PROJECT_ID="argo-813"
export REGION="asia-northeast3"
export ORGANIZATION_ID="38646727271"
export GITHUB_REPO="https://github.com/INESS-741209/Argo-813.git"
EOF

log "âœ… ëª¨ë“  ì„œë¹„ìŠ¤ ê³„ì • ìƒì„± ì™„ë£Œ"
log "ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ: source ${PROJECT_ROOT}/.env"

ğŸ“¦ Phase 1: Layer 1 - Omni-Contextual Core êµ¬ì¶•
[ëª…ë ¹ 1-1] BigQuery ë°ì´í„°ì…‹ ë° í…Œì´ë¸” ìƒì„±
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ scripts/setup_bigquery.sh íŒŒì¼ì„ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ë¼:
bash#!/bin/bash
# scripts/setup_bigquery.sh

source "$(dirname "$0")/../.env"
LOG_FILE="${PROJECT_ROOT}/argo_setup.log"

log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# ë°ì´í„°ì…‹ ìƒì„±
log "BigQuery ë°ì´í„°ì…‹ ìƒì„± ì¤‘..."
bq mk --dataset \
    --location=${REGION} \
    --description="ARGO Omni-Contextual Core Dataset" \
    ${PROJECT_ID}:omni_contextual_core || log "ë°ì´í„°ì…‹ì´ ì´ë¯¸ ì¡´ì¬í•¨"

# events í…Œì´ë¸” ìƒì„±
bq mk --table \
    ${PROJECT_ID}:omni_contextual_core.events \
    ./configs/bigquery_schemas/events_schema.json || log "events í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•¨"

# audit_logs í…Œì´ë¸” ìƒì„±
bq mk --table \
    ${PROJECT_ID}:omni_contextual_core.audit_logs \
    ./configs/bigquery_schemas/audit_schema.json || log "audit_logs í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•¨"

log "âœ… BigQuery ì„¤ì • ì™„ë£Œ"
[ëª…ë ¹ 1-2] BigQuery ìŠ¤í‚¤ë§ˆ íŒŒì¼ ìƒì„±
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ configs/bigquery_schemas/events_schema.json íŒŒì¼ì„ ìƒì„±í•˜ë¼:
json[
    {
        "name": "event_id",
        "type": "STRING",
        "mode": "REQUIRED",
        "description": "Unique event identifier"
    },
    {
        "name": "timestamp",
        "type": "TIMESTAMP",
        "mode": "REQUIRED",
        "description": "Event timestamp"
    },
    {
        "name": "source",
        "type": "STRING",
        "mode": "NULLABLE",
        "description": "Event source (local, gdrive, calendar, etc.)"
    },
    {
        "name": "content",
        "type": "JSON",
        "mode": "NULLABLE",
        "description": "Event content in JSON format"
    },
    {
        "name": "embeddings",
        "type": "FLOAT64",
        "mode": "REPEATED",
        "description": "Vector embeddings for semantic search"
    },
    {
        "name": "metadata",
        "type": "JSON",
        "mode": "NULLABLE",
        "description": "Additional metadata"
    }
]
[ëª…ë ¹ 1-3] Redis ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ scripts/setup_redis.sh íŒŒì¼ì„ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ë¼:
bash#!/bin/bash
# scripts/setup_redis.sh

source "$(dirname "$0")/../.env"
REDIS_INSTANCE="argo-cache"
TIER="basic"
SIZE="5"

log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

# Redis ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œë„
log "Redis ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘..."
if gcloud redis instances describe ${REDIS_INSTANCE} --region=${REGION} &>/dev/null; then
    log "Redis ì¸ìŠ¤í„´ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•¨"
    REDIS_HOST=$(gcloud redis instances describe ${REDIS_INSTANCE} --region=${REGION} --format="value(host)")
else
    gcloud redis instances create ${REDIS_INSTANCE} \
        --size=${SIZE} \
        --region=${REGION} \
        --tier=${TIER} \
        --redis-version=redis_6_x \
        --async
    
    log "Redis ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ìš”ì²­ë¨. í”„ë¡œë¹„ì €ë‹ ëŒ€ê¸° ì¤‘... (ì•½ 5-10ë¶„ ì†Œìš”)"
    
    # ìƒì„± ì™„ë£Œ ëŒ€ê¸°
    while true; do
        if gcloud redis instances describe ${REDIS_INSTANCE} --region=${REGION} &>/dev/null; then
            REDIS_HOST=$(gcloud redis instances describe ${REDIS_INSTANCE} --region=${REGION} --format="value(host)")
            log "âœ… Redis ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ: ${REDIS_HOST}"
            break
        fi
        sleep 30
    done
fi

# Redis ì—°ê²° ì •ë³´ ì €ì¥
echo "export REDIS_HOST=${REDIS_HOST}" >> ${PROJECT_ROOT}/.env
echo "export REDIS_PORT=6379" >> ${PROJECT_ROOT}/.env
[ëª…ë ¹ 1-4] Pub/Sub í† í”½ ë° êµ¬ë… ìƒì„±
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ scripts/setup_pubsub.py íŒŒì¼ì„ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ë¼:
python#!/usr/bin/env python3
# scripts/setup_pubsub.py

import os
import sys
from google.cloud import pubsub_v1
from google.api_core.exceptions import AlreadyExists

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
PROJECT_ID = "argo-813"
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.join(
    os.path.dirname(os.path.dirname(__file__)), "keys", "argo-main-key.json"
)

publisher = pubsub_v1.PublisherClient()
subscriber = pubsub_v1.SubscriberClient()

# í† í”½ ë° êµ¬ë… ì •ì˜
TOPICS_AND_SUBSCRIPTIONS = {
    "local-file-events": ["bigquery-streaming", "agent-processing"],
    "gdrive-events": ["bigquery-streaming", "agent-processing"],
    "agent-tasks": ["master-orchestrator", "specialist-agents"],
    "agent-results": ["result-processor", "audit-logger"],
    "system-commands": ["infrastructure-manager", "code-generator"]
}

def create_topic_and_subscriptions(topic_name, subscription_names):
    """í† í”½ê³¼ êµ¬ë…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    project_path = f"projects/{PROJECT_ID}"
    topic_path = publisher.topic_path(PROJECT_ID, topic_name)
    
    # í† í”½ ìƒì„±
    try:
        topic = publisher.create_topic(request={"name": topic_path})
        print(f"âœ… í† í”½ ìƒì„±ë¨: {topic.name}")
    except AlreadyExists:
        print(f"â„¹ï¸ í† í”½ì´ ì´ë¯¸ ì¡´ì¬í•¨: {topic_name}")
    
    # êµ¬ë… ìƒì„±
    for sub_name in subscription_names:
        subscription_path = subscriber.subscription_path(
            PROJECT_ID, f"{topic_name}-{sub_name}"
        )
        try:
            subscription = subscriber.create_subscription(
                request={
                    "name": subscription_path,
                    "topic": topic_path,
                    "ack_deadline_seconds": 300,  # 5ë¶„
                    "message_retention_duration": {"seconds": 86400}  # 1ì¼
                }
            )
            print(f"  âœ… êµ¬ë… ìƒì„±ë¨: {subscription.name}")
        except AlreadyExists:
            print(f"  â„¹ï¸ êµ¬ë…ì´ ì´ë¯¸ ì¡´ì¬í•¨: {sub_name}")

if __name__ == "__main__":
    print("=== Pub/Sub í† í”½ ë° êµ¬ë… ìƒì„± ì‹œì‘ ===")
    
    for topic, subscriptions in TOPICS_AND_SUBSCRIPTIONS.items():
        create_topic_and_subscriptions(topic, subscriptions)
    
    print("âœ… Pub/Sub ì„¤ì • ì™„ë£Œ")
[ëª…ë ¹ 1-5] Cloud Functions - ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ cloud_functions/data_ingestion/main.py íŒŒì¼ì„ ìƒì„±í•˜ë¼:
python# cloud_functions/data_ingestion/main.py

import os
import json
import base64
import uuid
from datetime import datetime
from google.cloud import bigquery
from google.cloud import pubsub_v1

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
bq_client = bigquery.Client()
publisher = pubsub_v1.PublisherClient()
PROJECT_ID = os.environ.get('PROJECT_ID', 'argo-813')
DATASET_ID = 'omni_contextual_core'
TABLE_ID = 'events'

def process_file_event(event, context):
    """
    íŒŒì¼ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ BigQueryì— ì €ì¥í•©ë‹ˆë‹¤.
    Cloud Storage íŠ¸ë¦¬ê±° ë˜ëŠ” Pub/Sub ë©”ì‹œì§€ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    try:
        # Pub/Sub ë©”ì‹œì§€ ë””ì½”ë”©
        if 'data' in event:
            message_data = base64.b64decode(event['data']).decode('utf-8')
            file_info = json.loads(message_data)
        else:
            # Cloud Storage ì´ë²¤íŠ¸ ì§ì ‘ ì²˜ë¦¬
            file_info = {
                'name': event.get('name', 'unknown'),
                'bucket': event.get('bucket', 'unknown'),
                'size': event.get('size', 0),
                'updated': event.get('updated', datetime.utcnow().isoformat())
            }
        
        # BigQuery í–‰ ë°ì´í„° ìƒì„±
        row = {
            'event_id': str(uuid.uuid4()),
            'timestamp': datetime.utcnow().isoformat(),
            'source': file_info.get('source', 'gcs'),
            'content': json.dumps(file_info),
            'embeddings': [],  # ì„ë² ë”©ì€ ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì²˜ë¦¬
            'metadata': json.dumps({
                'processed_by': 'data_ingestion_function',
                'version': '1.0'
            })
        }
        
        # BigQueryì— ì‚½ì…
        table_ref = f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"
        errors = bq_client.insert_rows_json(table_ref, [row])
        
        if errors:
            print(f"BigQuery ì‚½ì… ì˜¤ë¥˜: {errors}")
            return {'status': 'error', 'message': str(errors)}, 500
        
        print(f"âœ… ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: {row['event_id']}")
        
        # ì—ì´ì „íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ Pub/Subì— ë°œí–‰
        topic_path = publisher.topic_path(PROJECT_ID, 'agent-tasks')
        message = json.dumps({
            'event_id': row['event_id'],
            'action': 'PROCESS_NEW_DATA',
            'source': file_info.get('source', 'unknown')
        })
        
        future = publisher.publish(topic_path, message.encode('utf-8'))
        print(f"Pub/Sub ë©”ì‹œì§€ ë°œí–‰ë¨: {future.result()}")
        
        return {'status': 'success', 'event_id': row['event_id']}, 200
        
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {'status': 'error', 'message': str(e)}, 500

def sync_google_drive(request):
    """
    Google Drive ë™ê¸°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    HTTP íŠ¸ë¦¬ê±°ë¡œ í˜¸ì¶œë˜ê±°ë‚˜ ìŠ¤ì¼€ì¤„ëŸ¬ì— ì˜í•´ ì •ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    try:
        from googleapiclient.discovery import build
        from google.oauth2 import service_account
        
        # ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
        credentials = service_account.Credentials.from_service_account_file(
            '/etc/secrets/argo-main-key.json',
            scopes=['https://www.googleapis.com/auth/drive.readonly']
        )
        
        drive_service = build('drive', 'v3', credentials=credentials)
        
        # ìµœê·¼ ë³€ê²½ëœ íŒŒì¼ ì¡°íšŒ
        results = drive_service.files().list(
            pageSize=100,
            fields="files(id, name, mimeType, modifiedTime, size, parents)",
            orderBy="modifiedTime desc",
            q="modifiedTime > '2024-01-01T00:00:00'"
        ).execute()
        
        files = results.get('files', [])
        
        if not files:
            return {'status': 'success', 'message': 'No files to sync'}, 200
        
        # ê° íŒŒì¼ì„ BigQueryì— ì €ì¥
        table_ref = f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"
        rows_to_insert = []
        
        for file in files:
            row = {
                'event_id': str(uuid.uuid4()),
                'timestamp': datetime.utcnow().isoformat(),
                'source': 'gdrive',
                'content': json.dumps(file),
                'embeddings': [],
                'metadata': json.dumps({
                    'sync_batch': datetime.utcnow().isoformat(),
                    'file_count': len(files)
                })
            }
            rows_to_insert.append(row)
        
        errors = bq_client.insert_rows_json(table_ref, rows_to_insert)
        
        if errors:
            print(f"BigQuery ì‚½ì… ì˜¤ë¥˜: {errors}")
            return {'status': 'error', 'errors': errors}, 500
        
        print(f"âœ… {len(files)}ê°œ íŒŒì¼ ë™ê¸°í™” ì™„ë£Œ")
        return {'status': 'success', 'synced_files': len(files)}, 200
        
    except Exception as e:
        print(f"Drive ë™ê¸°í™” ì˜¤ë¥˜: {str(e)}")
        
        # Directorì—ê²Œ ìˆ˜ë™ ê°œì… ìš”ì²­
        if "insufficient authentication" in str(e).lower():
            return {
                'status': 'manual_intervention_required',
                'message': 'Google Drive API ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.',
                'action_required': 'GCP ì½˜ì†”ì—ì„œ Drive APIë¥¼ í™œì„±í™”í•˜ê³  OAuth ë™ì˜ í™”ë©´ì„ êµ¬ì„±í•´ì£¼ì„¸ìš”.',
                'instructions': 'https://console.cloud.google.com/apis/library/drive.googleapis.com'
            }, 403
        
        return {'status': 'error', 'message': str(e)}, 500
[ëª…ë ¹ 1-6] Cloud Functions ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ scripts/deploy_functions.sh íŒŒì¼ì„ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ë¼:
bash#!/bin/bash
# scripts/deploy_functions.sh

source "$(dirname "$0")/../.env"
FUNCTIONS_DIR="${PROJECT_ROOT}/cloud_functions"
RETRY_COUNT=0

log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

deploy_function() {
    local FUNCTION_NAME=$1
    local ENTRY_POINT=$2
    local TRIGGER_TYPE=$3
    local TRIGGER_RESOURCE=$4
    local SOURCE_DIR=$5
    
    log "í•¨ìˆ˜ ë°°í¬ ì¤‘: ${FUNCTION_NAME}"
    
    # requirements.txt ìƒì„±
    cat > ${SOURCE_DIR}/requirements.txt << EOF
google-cloud-bigquery==3.3.5
google-cloud-pubsub==2.18.4
google-cloud-storage==2.10.0
google-api-python-client==2.100.0
EOF
    
    # í•¨ìˆ˜ ë°°í¬
    if [ "$TRIGGER_TYPE" == "pubsub" ]; then
        gcloud functions deploy ${FUNCTION_NAME} \
            --runtime python310 \
            --trigger-topic ${TRIGGER_RESOURCE} \
            --entry-point ${ENTRY_POINT} \
            --source ${SOURCE_DIR} \
            --region ${REGION} \
            --service-account argo-functions@${PROJECT_ID}.iam.gserviceaccount.com \
            --set-env-vars PROJECT_ID=${PROJECT_ID} \
            --timeout 540s \
            --memory 512MB \
            --max-instances 10
    elif [ "$TRIGGER_TYPE" == "http" ]; then
        gcloud functions deploy ${FUNCTION_NAME} \
            --runtime python310 \
            --trigger-http \
            --allow-unauthenticated \
            --entry-point ${ENTRY_POINT} \
            --source ${SOURCE_DIR} \
            --region ${REGION} \
            --service-account argo-functions@${PROJECT_ID}.iam.gserviceaccount.com \
            --set-env-vars PROJECT_ID=${PROJECT_ID} \
            --timeout 540s \
            --memory 1GB
    fi
    
    if [ $? -eq 0 ]; then
        log "âœ… í•¨ìˆ˜ ë°°í¬ ì„±ê³µ: ${FUNCTION_NAME}"
    else
        log "âŒ í•¨ìˆ˜ ë°°í¬ ì‹¤íŒ¨: ${FUNCTION_NAME}"
        return 1
    fi
}

# ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ ë°°í¬
deploy_function \
    "process-file-events" \
    "process_file_event" \
    "pubsub" \
    "local-file-events" \
    "${FUNCTIONS_DIR}/data_ingestion"

deploy_function \
    "sync-google-drive" \
    "sync_google_drive" \
    "http" \
    "" \
    "${FUNCTIONS_DIR}/data_ingestion"

log "âœ… Cloud Functions ë°°í¬ ì™„ë£Œ"

ğŸ¤– Phase 2: Layer 2 - Multi-Agent Ideation Swarm
[ëª…ë ¹ 2-1] ADK ì—ì´ì „íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤ ìƒì„±
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ agents/base_agent.py íŒŒì¼ì„ ìƒì„±í•˜ë¼:
python# agents/base_agent.py

import os
import json
import redis
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional
from abc import ABC, abstractmethod
from google.cloud import pubsub_v1
from google.cloud import bigquery
import google.generativeai as genai

class BaseAgent(ABC):
    """ëª¨ë“  ARGO ì—ì´ì „íŠ¸ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, agent_name: str, agent_type: str):
        self.agent_name = agent_name
        self.agent_type = agent_type
        self.project_id = os.environ.get('PROJECT_ID', 'argo-813')
        
        # Redis ì—°ê²°
        redis_host = os.environ.get('REDIS_HOST', 'localhost')
        self.redis_client = redis.Redis(
            host=redis_host,
            port=6379,
            decode_responses=True
        )
        
        # Pub/Sub í´ë¼ì´ì–¸íŠ¸
        self.publisher = pubsub_v1.PublisherClient()
        self.subscriber = pubsub_v1.SubscriberClient()
        
        # BigQuery í´ë¼ì´ì–¸íŠ¸
        self.bq_client = bigquery.Client()
        
        # ì—ì´ì „íŠ¸ ìƒíƒœ ì´ˆê¸°í™”
        self.state = {
            'status': 'idle',
            'current_task': None,
            'task_history': []
        }
        
        # ì—ì´ì „íŠ¸ ë“±ë¡
        self._register_agent()
    
    def _register_agent(self):
        """Redisì— ì—ì´ì „íŠ¸ ë“±ë¡"""
        agent_key = f"agent:{self.agent_name}"
        agent_info = {
            'name': self.agent_name,
            'type': self.agent_type,
            'status': 'active',
            'registered_at': datetime.utcnow().isoformat(),
            'capabilities': self.get_capabilities()
        }
        self.redis_client.hset(agent_key, mapping=agent_info)
        self.redis_client.expire(agent_key, 3600)  # 1ì‹œê°„ TTL
        print(f"âœ… ì—ì´ì „íŠ¸ ë“±ë¡ë¨: {self.agent_name}")
    
    @abstractmethod
    def get_capabilities(self) -> List[str]:
        """ì—ì´ì „íŠ¸ì˜ ëŠ¥ë ¥ì„ ë°˜í™˜"""
        pass
    
    @abstractmethod
    async def process_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ì‘ì—…ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜"""
        pass
    
    def publish_message(self, topic: str, message: Dict[str, Any]):
        """Pub/Subì— ë©”ì‹œì§€ ë°œí–‰"""
        topic_path = self.publisher.topic_path(self.project_id, topic)
        message_bytes = json.dumps(message).encode('utf-8')
        future = self.publisher.publish(topic_path, message_bytes)
        return future.result()
    
    def query_bigquery(self, query: str) -> List[Dict]:
        """BigQuery ì¿¼ë¦¬ ì‹¤í–‰"""
        query_job = self.bq_client.query(query)
        results = query_job.result()
        return [dict(row) for row in results]
    
    def update_state(self, key: str, value: Any):
        """ì—ì´ì „íŠ¸ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        self.state[key] = value
        state_key = f"agent_state:{self.agent_name}"
        self.redis_client.hset(state_key, key, json.dumps(value))
    
    def get_shared_context(self, context_id: str) -> Dict:
        """ê³µìœ  ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ"""
        context_key = f"context:{context_id}"
        context = self.redis_client.get(context_key)
        return json.loads(context) if context else {}
    
    def set_shared_context(self, context_id: str, data: Dict):
        """ê³µìœ  ì»¨í…ìŠ¤íŠ¸ ì €ì¥"""
        context_key = f"context:{context_id}"
        self.redis_client.setex(
            context_key,
            3600,  # 1ì‹œê°„ TTL
            json.dumps(data)
        )
    
    async def run(self):
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ ë£¨í”„"""
        print(f"ğŸš€ {self.agent_name} ì‹œì‘ë¨")
        
        subscription_path = self.subscriber.subscription_path(
            self.project_id,
            f"agent-tasks-{self.agent_type}"
        )
        
        while True:
            try:
                # Pub/Subì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹ 
                response = self.subscriber.pull(
                    request={
                        "subscription": subscription_path,
                        "max_messages": 1
                    },
                    timeout=30
                )
                
                for msg in response.received_messages:
                    # ë©”ì‹œì§€ ì²˜ë¦¬
                    task_data = json.loads(msg.message.data.decode('utf-8'))
                    
                    if self._should_process_task(task_data):
                        self.update_state('status', 'processing')
                        self.update_state('current_task', task_data)
                        
                        # ì‘ì—… ì²˜ë¦¬
                        result = await self.process_task(task_data)
                        
                        # ê²°ê³¼ ë°œí–‰
                        self.publish_message('agent-results', {
                            'task_id': task_data.get('task_id'),
                            'agent': self.agent_name,
                            'result': result,
                            'timestamp': datetime.utcnow().isoformat()
                        })
                        
                        self.update_state('status', 'idle')
                        self.update_state('current_task', None)
                    
                    # ë©”ì‹œì§€ í™•ì¸
                    self.subscriber.acknowledge(
                        request={
                            "subscription": subscription_path,
                            "ack_ids": [msg.ack_id]
                        }
                    )
                
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"âŒ ì—ì´ì „íŠ¸ ì˜¤ë¥˜ ({self.agent_name}): {str(e)}")
                await asyncio.sleep(5)
    
    def _should_process_task(self, task: Dict) -> bool:
        """ì´ ì—ì´ì „íŠ¸ê°€ ì‘ì—…ì„ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨"""
        target_agent = task.get('target_agent')
        if target_agent and target_agent != self.agent_name:
            return False
        
        required_capabilities = task.get('required_capabilities', [])
        agent_capabilities = self.get_capabilities()
        
        return any(cap in agent_capabilities for cap in required_capabilities)
[ëª…ë ¹ 2-2] Master Orchestrator ì—ì´ì „íŠ¸ ìƒì„±
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ agents/master_orchestrator.py íŒŒì¼ì„ ìƒì„±í•˜ë¼:
python# agents/master_orchestrator.py

import os
import json
import uuid
from typing import Dict, Any, List
from agents.base_agent import BaseAgent
import google.generativeai as genai

class MasterOrchestratorAgent(BaseAgent):
    """ì „ì²´ ì—ì´ì „íŠ¸ ìŠ¤ì›œì„ ì§€íœ˜í•˜ëŠ” ë§ˆìŠ¤í„° AI"""
    
    def __init__(self):
        super().__init__("MasterOrchestrator", "orchestrator")
        
        # Gemini ëª¨ë¸ ì´ˆê¸°í™”
        genai.configure(api_key=os.environ.get('GEMINI_API_KEY'))
        self.model = genai.GenerativeModel('gemini-1.5-pro')
        
        # ë“±ë¡ëœ ì—ì´ì „íŠ¸ ëª©ë¡
        self.registered_agents = {}
        self._discover_agents()
    
    def get_capabilities(self) -> List[str]:
        return [
            "task_decomposition",
            "agent_coordination",
            "context_management",
            "result_synthesis",
            "decision_making"
        ]
    
    def _discover_agents(self):
        """Redisì—ì„œ í™œì„± ì—ì´ì „íŠ¸ ë°œê²¬"""
        agent_keys = self.redis_client.keys("agent:*")
        for key in agent_keys:
            agent_info = self.redis_client.hgetall(key)
            if agent_info.get('status') == 'active':
                agent_name = agent_info.get('name')
                self.registered_agents[agent_name] = agent_info
                print(f"ğŸ“Œ ì—ì´ì „íŠ¸ ë°œê²¬: {agent_name}")
    
    async def process_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """
        Directorì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³  í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ì „ë¬¸ ì—ì´ì „íŠ¸ì—ê²Œ ìœ„ì„
        """
        task_id = task.get('task_id', str(uuid.uuid4()))
        request = task.get('request', '')
        context = task.get('context', {})
        
        print(f"\nğŸ¯ Master Orchestrator ì‘ì—… ìˆ˜ì‹ : {request[:100]}...")
        
        # 1. ì‘ì—… ë¶„ì„ ë° ë¶„í•´
        decomposition = await self._decompose_task(request, context)
        
        # 2. ê° í•˜ìœ„ ì‘ì—…ì„ ì ì ˆí•œ ì—ì´ì „íŠ¸ì—ê²Œ í• ë‹¹
        subtask_results = []
        for subtask in decomposition['subtasks']:
            agent = self._select_best_agent(subtask)
            
            if agent:
                # ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—… í• ë‹¹
                subtask_message = {
                    'task_id': f"{task_id}_{subtask['id']}",
                    'parent_task_id': task_id,
                    'target_agent': agent,
                    'action': subtask['action'],
                    'parameters': subtask['parameters'],
                    'context': context
                }
                
                self.publish_message('agent-tasks', subtask_message)
                
                subtask_results.append({
                    'subtask_id': subtask['id'],
                    'assigned_to': agent,
                    'status': 'dispatched'
                })
                
                print(f"  â¡ï¸ ì‘ì—… í• ë‹¹: {subtask['action']} -> {agent}")
            else:
                print(f"  âš ï¸ ì í•©í•œ ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {subtask['action']}")
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ì €ì¥
        self.set_shared_context(task_id, {
            'original_request': request,
            'decomposition': decomposition,
            'subtask_assignments': subtask_results
        })
        
        return {
            'task_id': task_id,
            'status': 'orchestrated',
            'subtasks_count': len(decomposition['subtasks']),
            'assignments': subtask_results
        }
    
    async def _decompose_task(self, request: str, context: Dict) -> Dict:
        """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í•´"""
        
        prompt = f"""
        ë‹¹ì‹ ì€ ARGO ì‹œìŠ¤í…œì˜ Master Orchestratorì…ë‹ˆë‹¤.
        Directorì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”.
        
        Directorì˜ ìš”ì²­: {request}
        
        í˜„ì¬ ì»¨í…ìŠ¤íŠ¸: {json.dumps(context, ensure_ascii=False)}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ìœ í˜•:
        - UserContextAgent: ì‚¬ìš©ì ë°ì´í„° ë¶„ì„
        - CreativeAgent: ì°½ì˜ì  ì•„ì´ë””ì–´ ìƒì„±
        - TechnicalAgent: ê¸°ìˆ  êµ¬í˜„ ì„¤ê³„
        - DataAnalystAgent: ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "analysis": "ìš”ì²­ ë¶„ì„ ìš”ì•½",
            "subtasks": [
                {{
                    "id": "subtask_1",
                    "action": "ì‘ì—… ì„¤ëª…",
                    "required_capabilities": ["í•„ìš”í•œ ëŠ¥ë ¥"],
                    "parameters": {{}},
                    "dependencies": []
                }}
            ]
        }}
        """
        
        response = self.model.generate_content(prompt)
        
        try:
            # JSON íŒŒì‹±
            result = json.loads(response.text)
            return result
        except json.JSONDecodeError:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                "analysis": "ì‘ì—… ë¶„í•´ ì‹¤íŒ¨",
                "subtasks": [
                    {
                        "id": "default_1",
                        "action": request,
                        "required_capabilities": ["general"],
                        "parameters": {},
                        "dependencies": []
                    }
                ]
            }
    
    def _select_best_agent(self, subtask: Dict) -> str:
        """í•˜ìœ„ ì‘ì—…ì— ê°€ì¥ ì í•©í•œ ì—ì´ì „íŠ¸ ì„ íƒ"""
        required_capabilities = subtask.get('required_capabilities', [])
        
        best_agent = None
        best_score = 0
        
        for agent_name, agent_info in self.registered_agents.items():
            if agent_name == self.agent_name:
                continue
            
            agent_capabilities = json.loads(agent_info.get('capabilities', '[]'))
            
            # ëŠ¥ë ¥ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            score = sum(1 for cap in required_capabilities if cap in agent_capabilities)
            
            if score > best_score:
                best_score = score
                best_agent = agent_name
        
        return best_agent

# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
if __name__ == "__main__":
    import asyncio
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['REDIS_HOST'] = os.environ.get('REDIS_HOST', 'localhost')
    os.environ['GEMINI_API_KEY'] = os.environ.get('GEMINI_API_KEY', '')
    
    # Master Orchestrator ì‹¤í–‰
    orchestrator = MasterOrchestratorAgent()
    asyncio.run(orchestrator.run())
[ëª…ë ¹ 2-3] ì „ë¬¸ ì—ì´ì „íŠ¸ë“¤ ìƒì„±
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ agents/specialist_agents.py íŒŒì¼ì„ ìƒì„±í•˜ë¼:
python# agents/specialist_agents.py

import os
import json
import asyncio
from typing import Dict, Any, List
from agents.base_agent import BaseAgent
import google.generativeai as genai
from datetime import datetime, timedelta

class UserContextAgent(BaseAgent):
    """ì‚¬ìš©ìì˜ ì „ì²´ ë§¥ë½ì„ ì´í•´í•˜ëŠ” ì „ë¬¸ê°€"""
    
    def __init__(self):
        super().__init__("UserContextAnalyst", "context")
        genai.configure(api_key=os.environ.get('GEMINI_API_KEY'))
        self.model = genai.GenerativeModel('gemini-1.5-flash')
    
    def get_capabilities(self) -> List[str]:
        return ["user_analysis", "pattern_recognition", "context_extraction", "behavior_prediction"]
    
    async def process_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        action = task.get('action', '')
        parameters = task.get('parameters', {})
        
        if 'analyze' in action.lower():
            return await self._analyze_user_context(parameters)
        elif 'pattern' in action.lower():
            return await self._find_patterns(parameters)
        else:
            return {'error': f'Unknown action: {action}'}
    
    async def _analyze_user_context(self, params: Dict) -> Dict:
        """BigQueryì—ì„œ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ë¶„ì„"""
        user_id = params.get('user_id', 'director_iness')
        time_range = params.get('time_range', '7d')
        
        # BigQuery ì¿¼ë¦¬
        query = f"""
        SELECT 
            source,
            EXTRACT(HOUR FROM timestamp) as hour,
            EXTRACT(DAYOFWEEK FROM timestamp) as day_of_week,
            COUNT(*) as activity_count,
            ARRAY_AGG(DISTINCT JSON_EXTRACT_SCALAR(content, '$.type')) as content_types
        FROM `argo-813.omni_contextual_core.events`
        WHERE JSON_EXTRACT_SCALAR(metadata, '$.user_id') = @user_id
            AND timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_range})
        GROUP BY source, hour, day_of_week
        ORDER BY activity_count DESC
        LIMIT 100
        """
        
        results = self.query_bigquery(query)
        
        # Geminië¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        prompt = f"""
        ë‹¤ìŒ ì‚¬ìš©ì í™œë™ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” íŒ¨í„´ê³¼ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì„¸ìš”:
        {json.dumps(results, ensure_ascii=False)}
        
        ë‹¤ìŒì„ í¬í•¨í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”:
        1. ê°€ì¥ í™œë°œí•œ ì‹œê°„ëŒ€
        2. ì£¼ìš” í™œë™ ì†ŒìŠ¤
        3. í–‰ë™ íŒ¨í„´
        4. ì¶”ì²œ ì‚¬í•­
        """
        
        response = self.model.generate_content(prompt)
        
        return {
            'user_id': user_id,
            'analysis_period': time_range,
            'raw_data': results,
            'insights': response.text,
            'timestamp': datetime.utcnow().isoformat()
        }
    
    async def _find_patterns(self, params: Dict) -> Dict:
        """ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ íƒì§€"""
        # êµ¬í˜„ ì˜ˆì •
        return {'status': 'pattern_analysis_pending'}


class CreativeIdeationAgent(BaseAgent):
    """ì°½ì˜ì  ì•„ì´ë””ì–´ ìƒì„± ì „ë¬¸ê°€"""
    
    def __init__(self):
        super().__init__("CreativeIdeator", "creative")
        genai.configure(api_key=os.environ.get('GEMINI_API_KEY'))
        self.model = genai.GenerativeModel('gemini-1.5-pro')
    
    def get_capabilities(self) -> List[str]:
        return ["brainstorming", "creative_writing", "concept_generation", "innovation"]
    
    async def process_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        action = task.get('action', '')
        parameters = task.get('parameters', {})
        
        if 'generate' in action.lower() or 'create' in action.lower():
            return await self._generate_ideas(parameters)
        else:
            return {'error': f'Unknown action: {action}'}
    
    async def _generate_ideas(self, params: Dict) -> Dict:
        """ì°½ì˜ì  ì•„ì´ë””ì–´ ìƒì„±"""
        topic = params.get('topic', '')
        context = params.get('context', {})
        constraints = params.get('constraints', [])
        
        prompt = f"""
        ë‹¹ì‹ ì€ ARGO ì‹œìŠ¤í…œì˜ ì°½ì˜ì  ì•„ì´ë””ì–´ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        
        ì£¼ì œ: {topic}
        ì»¨í…ìŠ¤íŠ¸: {json.dumps(context, ensure_ascii=False)}
        ì œì•½ì‚¬í•­: {', '.join(constraints)}
        
        ë‹¤ìŒ ê´€ì ì—ì„œ í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”:
        1. ê¸°ìˆ ì  ì‹¤í˜„ ê°€ëŠ¥ì„±
        2. ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜
        3. ì‚¬ìš©ì ê²½í—˜
        4. ì°¨ë³„í™” ìš”ì†Œ
        
        ìµœì†Œ 5ê°œì˜ êµ¬ì²´ì ì¸ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ì„¸ìš”.
        """
        
        response = self.model.generate_content(prompt)
        
        return {
            'topic': topic,
            'ideas': response.text,
            'generation_method': 'gemini_pro',
            'timestamp': datetime.utcnow().isoformat()
        }


class TechnicalArchitectAgent(BaseAgent):
    """ê¸°ìˆ  ì•„í‚¤í…ì²˜ ì„¤ê³„ ì „ë¬¸ê°€"""
    
    def __init__(self):
        super().__init__("TechnicalArchitect", "technical")
        genai.configure(api_key=os.environ.get('GEMINI_API_KEY'))
        self.model = genai.GenerativeModel('gemini-1.5-pro')
    
    def get_capabilities(self) -> List[str]:
        return ["architecture_design", "code_review", "tech_stack_selection", "optimization"]
    
    async def process_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        action = task.get('action', '')
        parameters = task.get('parameters', {})
        
        if 'design' in action.lower():
            return await self._design_architecture(parameters)
        elif 'optimize' in action.lower():
            return await self._optimize_system(parameters)
        else:
            return {'error': f'Unknown action: {action}'}
    
    async def _design_architecture(self, params: Dict) -> Dict:
        """ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„"""
        requirements = params.get('requirements', '')
        constraints = params.get('constraints', {})
        
        prompt = f"""
        ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê¸°ìˆ  ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•˜ì„¸ìš”:
        
        ìš”êµ¬ì‚¬í•­: {requirements}
        ì œì•½ì‚¬í•­: {json.dumps(constraints, ensure_ascii=False)}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ìˆ  ìŠ¤íƒ:
        - GCP (BigQuery, Cloud Functions, Cloud Run, Vertex AI)
        - Redis, Neo4j
        - Python, TypeScript
        - Pulumi (IaC)
        
        ë‹¤ìŒì„ í¬í•¨í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„ë¥¼ ì œê³µí•˜ì„¸ìš”:
        1. ì‹œìŠ¤í…œ êµ¬ì„±ë„
        2. ë°ì´í„° íë¦„
        3. ê¸°ìˆ  ìŠ¤íƒ ì„ íƒ ê·¼ê±°
        4. í™•ì¥ì„± ê³ ë ¤ì‚¬í•­
        5. ë³´ì•ˆ ê³ ë ¤ì‚¬í•­
        """
        
        response = self.model.generate_content(prompt)
        
        return {
            'requirements': requirements,
            'architecture': response.text,
            'design_method': 'gemini_pro',
            'timestamp': datetime.utcnow().isoformat()
        }
    
    async def _optimize_system(self, params: Dict) -> Dict:
        """ì‹œìŠ¤í…œ ìµœì í™” ì œì•ˆ"""
        # êµ¬í˜„ ì˜ˆì •
        return {'status': 'optimization_pending'}


# ì—ì´ì „íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
async def run_all_agents():
    """ëª¨ë“  ì „ë¬¸ ì—ì´ì „íŠ¸ ì‹¤í–‰"""
    agents = [
        UserContextAgent(),
        CreativeIdeationAgent(),
        TechnicalArchitectAgent()
    ]
    
    # ê° ì—ì´ì „íŠ¸ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
    tasks = [agent.run() for agent in agents]
    await asyncio.gather(*tasks)

if __name__ == "__main__":
    import asyncio
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['REDIS_HOST'] = os.environ.get('REDIS_HOST', 'localhost')
    os.environ['GEMINI_API_KEY'] = os.environ.get('GEMINI_API_KEY', '')
    
    print("ğŸš€ ì „ë¬¸ ì—ì´ì „íŠ¸ë“¤ ì‹œì‘ ì¤‘...")
    asyncio.run(run_all_agents())

ğŸ“Š Phase 1-2 ê²€ì¦ ë° ê°ì‚¬
[ëª…ë ¹ V-1] Phase ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ scripts/validate_phase.py íŒŒì¼ì„ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ë¼:
python#!/usr/bin/env python3
# scripts/validate_phase.py

import os
import sys
import json
import subprocess
from datetime import datetime
from pathlib import Path
import hashlib

class PhaseValidator:
    def __init__(self, phase_number):
        self.phase_number = phase_number
        self.project_root = Path(__file__).parent.parent
        self.report = {
            'phase': phase_number,
            'timestamp': datetime.utcnow().isoformat(),
            'checks': [],
            'files_created': [],
            'services_deployed': [],
            'issues': [],
            'ready_for_commit': False
        }
    
    def check_file_exists(self, filepath, description):
        """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        full_path = self.project_root / filepath
        exists = full_path.exists()
        
        check_result = {
            'type': 'file_check',
            'path': str(filepath),
            'description': description,
            'status': 'PASS' if exists else 'FAIL'
        }
        
        if exists:
            # íŒŒì¼ í•´ì‹œ ê³„ì‚° (ë¬´ê²°ì„± í™•ì¸)
            with open(full_path, 'rb') as f:
                file_hash = hashlib.sha256(f.read()).hexdigest()
            check_result['hash'] = file_hash
            self.report['files_created'].append(str(filepath))
        else:
            self.report['issues'].append(f"Missing file: {filepath}")
        
        self.report['checks'].append(check_result)
        return exists
    
    def check_gcp_service(self, service_type, name, description):
        """GCP ì„œë¹„ìŠ¤ ë°°í¬ ìƒíƒœ í™•ì¸"""
        check_result = {
            'type': 'gcp_service',
            'service': service_type,
            'name': name,
            'description': description,
            'status': 'UNKNOWN'
        }
        
        try:
            if service_type == 'bigquery_dataset':
                cmd = f"bq ls -d --project_id=argo-813 | grep {name}"
            elif service_type == 'pubsub_topic':
                cmd = f"gcloud pubsub topics list --project=argo-813 | grep {name}"
            elif service_type == 'cloud_function':
                cmd = f"gcloud functions list --project=argo-813 --region=asia-northeast3 | grep {name}"
            elif service_type == 'redis':
                cmd = f"gcloud redis instances describe {name} --region=asia-northeast3 --project=argo-813"
            else:
                cmd = None
            
            if cmd:
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
                if result.returncode == 0:
                    check_result['status'] = 'DEPLOYED'
                    self.report['services_deployed'].append(f"{service_type}:{name}")
                else:
                    check_result['status'] = 'NOT_FOUND'
                    self.report['issues'].append(f"Service not deployed: {service_type}:{name}")
        except Exception as e:
            check_result['status'] = 'ERROR'
            check_result['error'] = str(e)
            self.report['issues'].append(f"Error checking {service_type}:{name} - {str(e)}")
        
        self.report['checks'].append(check_result)
        return check_result['status'] == 'DEPLOYED'
    
    def validate_phase_1(self):
        """Phase 1 ê²€ì¦"""
        print("=== Phase 1: Omni-Contextual Core ê²€ì¦ ì‹œì‘ ===")
        
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        files_to_check = [
            ('scripts/init_project.sh', 'í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸'),
            ('scripts/create_service_accounts.sh', 'ì„œë¹„ìŠ¤ ê³„ì • ìƒì„± ìŠ¤í¬ë¦½íŠ¸'),
            ('scripts/setup_bigquery.sh', 'BigQuery ì„¤ì • ìŠ¤í¬ë¦½íŠ¸'),
            ('scripts/setup_redis.sh', 'Redis ì„¤ì • ìŠ¤í¬ë¦½íŠ¸'),
            ('scripts/setup_pubsub.py', 'Pub/Sub ì„¤ì • ìŠ¤í¬ë¦½íŠ¸'),
            ('cloud_functions/data_ingestion/main.py', 'ë°ì´í„° ìˆ˜ì§‘ Cloud Function'),
            ('configs/bigquery_schemas/events_schema.json', 'BigQuery ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ'),
            ('.env', 'í™˜ê²½ ë³€ìˆ˜ íŒŒì¼'),
        ]
        
        for filepath, desc in files_to_check:
            self.check_file_exists(filepath, desc)
        
        # GCP ì„œë¹„ìŠ¤ í™•ì¸
        self.check_gcp_service('bigquery_dataset', 'omni_contextual_core', 'BigQuery ë°ì´í„°ì…‹')
        self.check_gcp_service('pubsub_topic', 'local-file-events', 'Pub/Sub í† í”½')
        self.check_gcp_service('pubsub_topic', 'agent-tasks', 'Pub/Sub í† í”½')
        self.check_gcp_service('redis', 'argo-cache', 'Redis ì¸ìŠ¤í„´ìŠ¤')
        
        # ì„œë¹„ìŠ¤ ê³„ì • í‚¤ í™•ì¸
        key_files = [
            'keys/argo-main-key.json',
            'keys/argo-functions-key.json',
            'keys/argo-bigquery-key.json'
        ]
        
        for key_file in key_files:
            self.check_file_exists(key_file, f'ì„œë¹„ìŠ¤ ê³„ì • í‚¤: {key_file}')
    
    def validate_phase_2(self):
        """Phase 2 ê²€ì¦"""
        print("=== Phase 2: Multi-Agent Swarm ê²€ì¦ ì‹œì‘ ===")
        
        files_to_check = [
            ('agents/base_agent.py', 'ì—ì´ì „íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤'),
            ('agents/master_orchestrator.py', 'Master Orchestrator'),
            ('agents/specialist_agents.py', 'ì „ë¬¸ ì—ì´ì „íŠ¸ë“¤'),
        ]
        
        for filepath, desc in files_to_check:
            self.check_file_exists(filepath, desc)
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
        # TODO: ì—ì´ì „íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¶”ê°€
    
    def generate_report(self):
        """ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        total_checks = len(self.report['checks'])
        passed_checks = sum(1 for c in self.report['checks'] if c['status'] in ['PASS', 'DEPLOYED'])
        
        self.report['summary'] = {
            'total_checks': total_checks,
            'passed': passed_checks,
            'failed': total_checks - passed_checks,
            'pass_rate': f"{(passed_checks/total_checks*100):.1f}%" if total_checks > 0 else "0%"
        }
        
        self.report['ready_for_commit'] = (passed_checks == total_checks) and len(self.report['issues']) == 0
        
        # ë³´ê³ ì„œ ì €ì¥
        report_file = self.project_root / f"reports/phase_{self.phase_number}_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w') as f:
            json.dump(self.report, f, indent=2, ensure_ascii=False)
        
        # ì½˜ì†” ì¶œë ¥
        print("\n" + "="*60)
        print(f"ğŸ“Š Phase {self.phase_number} ê²€ì¦ ë³´ê³ ì„œ")
        print("="*60)
        print(f"ì´ ê²€ì‚¬ í•­ëª©: {total_checks}")
        print(f"ì„±ê³µ: {passed_checks}")
        print(f"ì‹¤íŒ¨: {total_checks - passed_checks}")
        print(f"í†µê³¼ìœ¨: {self.report['summary']['pass_rate']}")
        
        if self.report['issues']:
            print("\nâš ï¸ ë°œê²¬ëœ ë¬¸ì œ:")
            for issue in self.report['issues']:
                print(f"  - {issue}")
        
        if self.report['ready_for_commit']:
            print("\nâœ… Phase ê²€ì¦ ì™„ë£Œ! Git ì»¤ë°‹ ì¤€ë¹„ ì™„ë£Œ")
            print("\në‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì»¤ë°‹ì„ ìš”ì²­í•˜ì„¸ìš”:")
            print(f"  python scripts/request_commit.py {self.phase_number}")
        else:
            print("\nâŒ Phase ê²€ì¦ ì‹¤íŒ¨. ìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            
            # Directorì—ê²Œ ìˆ˜ë™ ê°œì… ìš”ì²­
            manual_tasks_file = self.project_root / "manual_tasks.txt"
            with open(manual_tasks_file, 'a') as f:
                f.write(f"\n[{datetime.now().isoformat()}] Phase {self.phase_number} ê²€ì¦ ì‹¤íŒ¨\n")
                for issue in self.report['issues']:
                    f.write(f"  - {issue}\n")
            
            print(f"\nğŸ“ ìˆ˜ë™ ì‘ì—… í•„ìš” ì‚¬í•­ì´ {manual_tasks_file}ì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return self.report['ready_for_commit']

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python validate_phase.py <phase_number>")
        sys.exit(1)
    
    phase_number = int(sys.argv[1])
    validator = PhaseValidator(phase_number)
    
    if phase_number == 1:
        validator.validate_phase_1()
    elif phase_number == 2:
        validator.validate_phase_2()
    else:
        print(f"Unknown phase: {phase_number}")
        sys.exit(1)
    
    success = validator.generate_report()
    sys.exit(0 if success else 1)
[ëª…ë ¹ V-2] Git ì»¤ë°‹ ìš”ì²­ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ scripts/request_commit.py íŒŒì¼ì„ ìƒì„±í•˜ë¼:
python#!/usr/bin/env python3
# scripts/request_commit.py

import os
import sys
import json
import subprocess
from datetime import datetime
from pathlib import Path

def request_commit_approval(phase_number):
    project_root = Path(__file__).parent.parent
    
    # ìµœì‹  ê²€ì¦ ë³´ê³ ì„œ ì°¾ê¸°
    reports_dir = project_root / "reports"
    report_files = list(reports_dir.glob(f"phase_{phase_number}_validation_*.json"))
    
    if not report_files:
        print(f"âŒ Phase {phase_number}ì˜ ê²€ì¦ ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € python scripts/validate_phase.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return False
    
    latest_report = max(report_files, key=lambda p: p.stat().st_mtime)
    
    with open(latest_report, 'r') as f:
        report = json.load(f)
    
    if not report['ready_for_commit']:
        print("âŒ Phase ê²€ì¦ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return False
    
    # Git ìƒíƒœ í™•ì¸
    result = subprocess.run(['git', 'status', '--porcelain'], capture_output=True, text=True)
    changed_files = result.stdout.strip().split('\n') if result.stdout.strip() else []
    
    print("\n" + "="*60)
    print(f"ğŸ”„ Phase {phase_number} Git ì»¤ë°‹ ìŠ¹ì¸ ìš”ì²­")
    print("="*60)
    
    print(f"\nğŸ“Š ê²€ì¦ ìš”ì•½:")
    print(f"  - ì´ ê²€ì‚¬: {report['summary']['total_checks']}")
    print(f"  - í†µê³¼: {report['summary']['passed']}")
    print(f"  - í†µê³¼ìœ¨: {report['summary']['pass_rate']}")
    
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ ({len(report['files_created'])}ê°œ):")
    for file in report['files_created'][:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
        print(f"  - {file}")
    if len(report['files_created']) > 10:
        print(f"  ... ì™¸ {len(report['files_created']) - 10}ê°œ")
    
    print(f"\nğŸš€ ë°°í¬ëœ ì„œë¹„ìŠ¤ ({len(report['services_deployed'])}ê°œ):")
    for service in report['services_deployed']:
        print(f"  - {service}")
    
    print(f"\nğŸ“ ë³€ê²½ëœ íŒŒì¼ ({len(changed_files)}ê°œ):")
    for file in changed_files[:10]:
        print(f"  {file}")
    if len(changed_files) > 10:
        print(f"  ... ì™¸ {len(changed_files) - 10}ê°œ")
    
    print("\n" + "-"*60)
    print("Directorë‹˜, ìœ„ ë‚´ìš©ì„ ê²€í† í•˜ì‹œê³  ì»¤ë°‹ì„ ìŠ¹ì¸í•´ì£¼ì„¸ìš”.")
    print("\nìŠ¹ì¸í•˜ì‹œë ¤ë©´ 'yes'ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê±°ë¶€: 'no'):")
    
    approval = input("> ").strip().lower()
    
    if approval == 'yes':
        # Git ì»¤ë°‹ ì‹¤í–‰
        commit_message = f"Phase {phase_number}: {get_phase_description(phase_number)} - Completed"
        
        subprocess.run(['git', 'add', '.'])
        subprocess.run(['git', 'commit', '-m', commit_message])
        
        print(f"\nâœ… Phase {phase_number} ì»¤ë°‹ ì™„ë£Œ!")
        print("ë‹¤ìŒ Phaseë¥¼ ì§„í–‰í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë‹¤ìŒ Phase ì•ˆë‚´
        next_phase = phase_number + 1
        print(f"\në‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ Phase {next_phase}ë¥¼ ì‹œì‘í•˜ì„¸ìš”:")
        print(f"  python scripts/start_phase.py {next_phase}")
        
        return True
    else:
        print("\nì»¤ë°‹ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False

def get_phase_description(phase_number):
    descriptions = {
        0: "Initial Setup & Service Accounts",
        1: "Omni-Contextual Core",
        2: "Multi-Agent Ideation Swarm",
        3: "Creative & Analytical Unit",
        4: "Autonomous Development Arm",
        5: "Multi-tenant Architecture"
    }
    return descriptions.get(phase_number, f"Phase {phase_number}")

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python request_commit.py <phase_number>")
        sys.exit(1)
    
    phase_number = int(sys.argv[1])
    success = request_commit_approval(phase_number)
    sys.exit(0 if success else 1)

ğŸš€ ìµœì¢… ì‹¤í–‰ ëª…ë ¹ - ì»¤ì„œ AI ì „ìš©
[AUTORUN] ì™„ì „ ìë™ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ìŒ ë‚´ìš©ìœ¼ë¡œ AUTORUN.sh íŒŒì¼ì„ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ë¼:
bash#!/bin/bash
# AUTORUN.sh - ARGO ì‹œìŠ¤í…œ ì™„ì „ ìë™ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

set -e
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd $PROJECT_ROOT

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

# Git ì €ì¥ì†Œ ì´ˆê¸°í™”
if [ ! -d ".git" ]; then
    log "Git ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘..."
    git init
    git remote add origin https://github.com/INESS-741209/Argo-813.git
fi

# Phase 0: ì´ˆê¸° ì„¤ì •
log "ğŸš€ Phase 0: í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ì‹œì‘"
chmod +x scripts/init_project.sh
./scripts/init_project.sh

log "ğŸ” ì„œë¹„ìŠ¤ ê³„ì • ìƒì„± ì¤‘..."
chmod +x scripts/create_service_accounts.sh
./scripts/create_service_accounts.sh

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
source .env

# Phase 1: Omni-Contextual Core
log "ğŸ“¦ Phase 1: Omni-Contextual Core êµ¬ì¶• ì‹œì‘"

log "BigQuery ì„¤ì • ì¤‘..."
chmod +x scripts/setup_bigquery.sh
./scripts/setup_bigquery.sh

log "Redis ì„¤ì • ì¤‘..."
chmod +x scripts/setup_redis.sh
./scripts/setup_redis.sh

log "Pub/Sub ì„¤ì • ì¤‘..."
python3 scripts/setup_pubsub.py

log "Cloud Functions ë°°í¬ ì¤‘..."
chmod +x scripts/deploy_functions.sh
./scripts/deploy_functions.sh

# Phase 1 ê²€ì¦
log "ğŸ” Phase 1 ê²€ì¦ ì¤‘..."
if python3 scripts/validate_phase.py 1; then
    log "âœ… Phase 1 ê²€ì¦ ì„±ê³µ!"
    
    # ìë™ ì»¤ë°‹ (Director ìŠ¹ì¸ ëŒ€ê¸°)
    warning "Directorë‹˜ì˜ ì»¤ë°‹ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
    echo "yes" | python3 scripts/request_commit.py 1
else
    error "Phase 1 ê²€ì¦ ì‹¤íŒ¨. manual_tasks.txtë¥¼ í™•ì¸í•˜ì„¸ìš”."
    exit 1
fi

# Phase 2: Multi-Agent Swarm
log "ğŸ¤– Phase 2: Multi-Agent Swarm êµ¬ì¶• ì‹œì‘"

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# ì—ì´ì „íŠ¸ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
log "ì—ì´ì „íŠ¸ ì‹œì‘ ì¤‘..."
python3 agents/master_orchestrator.py &
MASTER_PID=$!
python3 agents/specialist_agents.py &
SPECIALIST_PID=$!

sleep 10

# Phase 2 ê²€ì¦
log "ğŸ” Phase 2 ê²€ì¦ ì¤‘..."
if python3 scripts/validate_phase.py 2; then
    log "âœ… Phase 2 ê²€ì¦ ì„±ê³µ!"
    echo "yes" | python3 scripts/request_commit.py 2
else
    error "Phase 2 ê²€ì¦ ì‹¤íŒ¨."
    kill $MASTER_PID $SPECIALIST_PID
    exit 1
fi

log "=" * 60
log "ğŸ‰ ARGO ì‹œìŠ¤í…œ Phase 1-2 êµ¬ì¶• ì™„ë£Œ!"
log "=" * 60
log ""
log "ë‹¤ìŒ ë‹¨ê³„:"
log "1. manual_tasks.txt íŒŒì¼ì„ í™•ì¸í•˜ì—¬ ìˆ˜ë™ ì‘ì—… ì™„ë£Œ"
log "2. Google Drive API ê¶Œí•œ ì„¤ì •"
log "3. Vertex AI API í‚¤ ì„¤ì •"
log "4. Phase 3 ì‹œì‘: ./scripts/start_phase.py 3"
