/**
 * ARGO Layer 1 Phase 1: OpenAI Embeddings Integration
 * ë¸”ë£¨í”„ë¦°íŠ¸: "ì„ì‹œ MD5 í•´ì‹œë¥¼ ì‹¤ì œ ì˜ë¯¸ë¡ ì  ì„ë² ë”©ìœ¼ë¡œ êµì²´"
 * ëª©í‘œ: ì§„ì •í•œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í˜„
 */

import OpenAI from 'openai';
import * as fs from 'fs/promises';

interface EmbeddingCache {
  [key: string]: {
    embedding: number[];
    timestamp: Date;
    version: string;
  };
}

interface EmbeddingRequest {
  text: string;
  model?: string;
  maxTokens?: number;
}

interface EmbeddingResult {
  embedding: number[];
  tokens: number;
  model: string;
  cached: boolean;
}

/**
 * OpenAI Embeddings Service
 * Phase 1 í•µì‹¬: MD5 í•´ì‹œë¥¼ ì‹¤ì œ ì˜ë¯¸ë¡ ì  ì„ë² ë”©ìœ¼ë¡œ êµì²´
 */
class EmbeddingService {
  private openai: OpenAI;
  private cache: EmbeddingCache;
  private cacheFile: string;
  private defaultModel: string;
  private maxCacheSize: number;
  private apiCallCount: number;
  private costTracker: number; // USD

  constructor(apiKey?: string) {
    // API í‚¤ëŠ” .env íŒŒì¼ì—ì„œ ë¡œë“œ
    this.openai = new OpenAI({
      apiKey: apiKey || process.env.OPENAI_API_KEY
    });
    this.cache = {};
    this.cacheFile = 'C:\\Argo-813\\data\\embeddings-cache.json';
    this.defaultModel = 'text-embedding-3-large'; // ìµœì‹  ëª¨ë¸
    this.maxCacheSize = 10000; // ìµœëŒ€ 10,000ê°œ ì„ë² ë”© ìºì‹œ
    this.apiCallCount = 0;
    this.costTracker = 0;
    
    this.loadCache();
  }

  /**
   * í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ë¡ ì  ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
   * ìºì‹±ì„ í†µí•œ API ë¹„ìš© ìµœì í™”
   */
  async getEmbedding(request: EmbeddingRequest): Promise<EmbeddingResult> {
    const { text, model = this.defaultModel, maxTokens = 8192 } = request;
    
    // í…ìŠ¤íŠ¸ ì •ê·œí™” ë° í† í° ì œí•œ
    const normalizedText = this.normalizeText(text, maxTokens);
    const cacheKey = await this.generateCacheKey(normalizedText, model);
    
    // ìºì‹œ í™•ì¸
    const cached = this.cache[cacheKey];
    if (cached && this.isCacheValid(cached)) {
      return {
        embedding: cached.embedding,
        tokens: this.estimateTokens(normalizedText),
        model: model,
        cached: true
      };
    }
    
    try {
      // OpenAI API í˜¸ì¶œ
      console.log(`ğŸ”„ OpenAI Embeddings API í˜¸ì¶œ ì¤‘... (${normalizedText.length} chars)`);
      
      const response = await this.openai.embeddings.create({
        model: model,
        input: normalizedText
      });
      
      const embedding = response.data[0].embedding;
      const tokens = response.usage?.total_tokens || this.estimateTokens(normalizedText);
      
      // API ì‚¬ìš©ëŸ‰ ì¶”ì 
      this.apiCallCount++;
      this.costTracker += this.calculateCost(tokens, model);
      
      // ìºì‹œ ì €ì¥
      this.cache[cacheKey] = {
        embedding,
        timestamp: new Date(),
        version: '1.0'
      };
      
      // ìºì‹œ í¬ê¸° ê´€ë¦¬
      await this.manageCacheSize();
      await this.saveCache();
      
      console.log(`âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ (${tokens} tokens, $${this.calculateCost(tokens, model).toFixed(4)})`);
      
      return {
        embedding,
        tokens,
        model,
        cached: false
      };
      
    } catch (error: any) {
      console.error('âŒ OpenAI API ì˜¤ë¥˜:', error.response?.data || error.message);
      
      // í´ë°±: ë¡œì»¬ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© (Phase 0 ë°©ì‹)
      console.log('ğŸ”„ ë¡œì»¬ í´ë°± ì„ë² ë”© ì‚¬ìš©');
      return {
        embedding: await this.generateFallbackEmbedding(normalizedText),
        tokens: this.estimateTokens(normalizedText),
        model: 'local-fallback',
        cached: false
      };
    }
  }

  /**
   * ë°°ì¹˜ ì„ë² ë”© ìƒì„± (íš¨ìœ¨ì„± ìµœì í™”)
   */
  async getBatchEmbeddings(texts: string[], model?: string): Promise<EmbeddingResult[]> {
    const batchSize = 100; // OpenAI ë°°ì¹˜ ì œí•œ
    const results: EmbeddingResult[] = [];
    
    for (let i = 0; i < texts.length; i += batchSize) {
      const batch = texts.slice(i, i + batchSize);
      console.log(`ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬: ${i + 1}-${Math.min(i + batchSize, texts.length)}/${texts.length}`);
      
      // ìºì‹œë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ë§Œ API í˜¸ì¶œ
      const uncachedTexts = [];
      for (const text of batch) {
        const cacheKey = await this.generateCacheKey(text, model || this.defaultModel);
        if (!this.cache[cacheKey] || !this.isCacheValid(this.cache[cacheKey])) {
          uncachedTexts.push(text);
        }
      }
      
      if (uncachedTexts.length > 0) {
        try {
          const response = await this.openai.embeddings.create({
            model: model || this.defaultModel,
            input: uncachedTexts
          });
          
          // ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
          for (let index = 0; index < response.data.length; index++) {
            const item = response.data[index];
            const text = uncachedTexts[index];
            const cacheKey = await this.generateCacheKey(text, model || this.defaultModel);
            
            this.cache[cacheKey] = {
              embedding: item.embedding,
              timestamp: new Date(),
              version: '1.0'
            };
          }
          
          this.apiCallCount++;
          this.costTracker += this.calculateCost(
            response.usage?.total_tokens || 0, 
            model || this.defaultModel
          );
          
        } catch (error) {
          console.warn('ë°°ì¹˜ API í˜¸ì¶œ ì‹¤íŒ¨, ê°œë³„ ì²˜ë¦¬ë¡œ ì „í™˜:', error);
        }
      }
      
      // ì „ì²´ ë°°ì¹˜ ê²°ê³¼ ìˆ˜ì§‘
      for (const text of batch) {
        const result = await this.getEmbedding({ text, model });
        results.push(result);
      }
      
      // API ë ˆì´íŠ¸ ì œí•œ ê³ ë ¤ (ì ì‹œ ëŒ€ê¸°)
      if (uncachedTexts.length > 0) {
        await this.sleep(100);
      }
    }
    
    await this.saveCache();
    return results;
  }

  /**
   * ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ë²¡í„°í™”ëœ ê³ ì„±ëŠ¥ êµ¬í˜„)
   */
  cosineSimilarity(embedding1: number[], embedding2: number[]): number {
    if (embedding1.length !== embedding2.length) {
      throw new Error('ì„ë² ë”© ì°¨ì›ì´ ë‹¤ë¦…ë‹ˆë‹¤');
    }
    
    let dotProduct = 0;
    let norm1 = 0;
    let norm2 = 0;
    
    for (let i = 0; i < embedding1.length; i++) {
      dotProduct += embedding1[i] * embedding2[i];
      norm1 += embedding1[i] * embedding1[i];
      norm2 += embedding2[i] * embedding2[i];
    }
    
    const magnitude1 = Math.sqrt(norm1);
    const magnitude2 = Math.sqrt(norm2);
    
    if (magnitude1 === 0 || magnitude2 === 0) {
      return 0;
    }
    
    return dotProduct / (magnitude1 * magnitude2);
  }

  /**
   * ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (K-NN)
   */
  findSimilarDocuments(
    queryEmbedding: number[], 
    documentEmbeddings: Array<{id: string, embedding: number[]}>, 
    topK: number = 10,
    threshold: number = 0.7
  ): Array<{id: string, similarity: number}> {
    
    const similarities = documentEmbeddings.map(doc => ({
      id: doc.id,
      similarity: this.cosineSimilarity(queryEmbedding, doc.embedding)
    }));
    
    return similarities
      .filter(item => item.similarity >= threshold)
      .sort((a, b) => b.similarity - a.similarity)
      .slice(0, topK);
  }

  /**
   * ì„ë² ë”© í’ˆì§ˆ í‰ê°€
   */
  evaluateEmbeddingQuality(embedding: number[]): {
    dimension: number;
    magnitude: number;
    sparsity: number; // 0ì— ê°€ê¹Œìš´ ê°’ì˜ ë¹„ìœ¨
    quality: 'excellent' | 'good' | 'fair' | 'poor';
  } {
    const dimension = embedding.length;
    const magnitude = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
    const zeroCount = embedding.filter(val => Math.abs(val) < 0.001).length;
    const sparsity = zeroCount / dimension;
    
    let quality: 'excellent' | 'good' | 'fair' | 'poor';
    if (magnitude > 0.8 && sparsity < 0.1) quality = 'excellent';
    else if (magnitude > 0.6 && sparsity < 0.2) quality = 'good';
    else if (magnitude > 0.4 && sparsity < 0.4) quality = 'fair';
    else quality = 'poor';
    
    return { dimension, magnitude, sparsity, quality };
  }

  /**
   * ë¹„ìš© ë° ì‚¬ìš©ëŸ‰ í†µê³„
   */
  getUsageStats() {
    return {
      apiCalls: this.apiCallCount,
      totalCost: parseFloat(this.costTracker.toFixed(4)),
      cacheSize: Object.keys(this.cache).length,
      cacheHitRate: this.calculateCacheHitRate(),
      avgCostPerCall: this.apiCallCount > 0 ? this.costTracker / this.apiCallCount : 0
    };
  }

  // ======= Private Methods =======

  private normalizeText(text: string, maxTokens: number): string {
    // í…ìŠ¤íŠ¸ ì •ë¦¬ ë° í† í° ì œí•œ
    let normalized = text
      .replace(/\s+/g, ' ')
      .replace(/[^\w\s\-_.]/g, ' ')
      .trim();
    
    // ëŒ€ëµì ì¸ í† í° ì œí•œ (1 í† í° â‰ˆ 4 ë¬¸ì)
    const maxChars = maxTokens * 4;
    if (normalized.length > maxChars) {
      normalized = normalized.substring(0, maxChars);
    }
    
    return normalized;
  }

  private async generateCacheKey(text: string, model: string): Promise<string> {
    // í…ìŠ¤íŠ¸ì™€ ëª¨ë¸ ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±
    const crypto = await import('crypto');
    return crypto
      .createHash('md5')
      .update(`${text}:${model}`)
      .digest('hex');
  }

  private isCacheValid(cached: { timestamp: Date; version: string }): boolean {
    const maxAge = 7 * 24 * 60 * 60 * 1000; // 7ì¼
    const age = Date.now() - cached.timestamp.getTime();
    return age < maxAge && cached.version === '1.0';
  }

  private estimateTokens(text: string): number {
    // ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (1 í† í° â‰ˆ 4 ë¬¸ì)
    return Math.ceil(text.length / 4);
  }

  private calculateCost(tokens: number, model: string): number {
    // OpenAI ê°€ê²©í‘œ (2024ë…„ ê¸°ì¤€)
    const pricePerToken = {
      'text-embedding-3-large': 0.00013 / 1000,    // $0.13 per 1M tokens
      'text-embedding-3-small': 0.00002 / 1000,    // $0.02 per 1M tokens
      'text-embedding-ada-002': 0.00010 / 1000     // $0.10 per 1M tokens
    };
    
    return tokens * (pricePerToken[model as keyof typeof pricePerToken] || pricePerToken['text-embedding-3-large']);
  }

  private async generateFallbackEmbedding(text: string): Promise<number[]> {
    // Phase 0 ë°©ì‹ í´ë°± (MD5 ê¸°ë°˜)
    const crypto = await import('crypto');
    const hash = crypto.createHash('md5').update(text).digest();
    const embedding = new Array(1536); // OpenAI ì„ë² ë”©ê³¼ ê°™ì€ ì°¨ì›
    
    for (let i = 0; i < 1536; i++) {
      embedding[i] = (hash[i % hash.length] - 128) / 128;
    }
    
    return embedding;
  }

  private async manageCacheSize(): Promise<void> {
    const keys = Object.keys(this.cache);
    
    if (keys.length > this.maxCacheSize) {
      // ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©ë“¤ ì œê±°
      const sortedKeys = keys.sort((a, b) => 
        this.cache[a].timestamp.getTime() - this.cache[b].timestamp.getTime()
      );
      
      const toRemove = sortedKeys.slice(0, keys.length - this.maxCacheSize);
      toRemove.forEach(key => delete this.cache[key]);
      
      console.log(`ğŸ§¹ ìºì‹œ ì •ë¦¬: ${toRemove.length}ê°œ í•­ëª© ì œê±°`);
    }
  }

  private async loadCache(): Promise<void> {
    try {
      // ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
      await fs.mkdir('C:\\Argo-813\\data', { recursive: true });
      
      const data = await fs.readFile(this.cacheFile, 'utf-8');
      this.cache = JSON.parse(data);
      
      // Date ê°ì²´ ë³µì›
      Object.values(this.cache).forEach(item => {
        item.timestamp = new Date(item.timestamp);
      });
      
      console.log(`ğŸ“‚ ì„ë² ë”© ìºì‹œ ë¡œë“œ: ${Object.keys(this.cache).length}ê°œ í•­ëª©`);
      
    } catch (error) {
      console.log('ğŸ“ ìƒˆë¡œìš´ ì„ë² ë”© ìºì‹œ ìƒì„±');
      this.cache = {};
    }
  }

  private async saveCache(): Promise<void> {
    try {
      await fs.writeFile(
        this.cacheFile, 
        JSON.stringify(this.cache, null, 2)
      );
    } catch (error) {
      console.warn('ìºì‹œ ì €ì¥ ì‹¤íŒ¨:', error);
    }
  }

  private calculateCacheHitRate(): number {
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” íˆíŠ¸/ë¯¸ìŠ¤ ì¹´ìš´í„° í•„ìš”
    return 0.75; // ì„ì‹œê°’
  }

  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

export {
  EmbeddingService,
  EmbeddingRequest,
  EmbeddingResult
};