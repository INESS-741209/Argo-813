"""
í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì €
ARGO Phase 2: ë°ì´í„° ì¼ê´€ì„± ë¬¸ì œ í•´ê²° ë° ì´ë²¤íŠ¸ ê¸°ë°˜ ë™ê¸°í™”
"""

import asyncio
import json
import logging
import time
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import uuid
from collections import defaultdict, deque
import hashlib

logger = logging.getLogger(__name__)

class SyncOperationType(Enum):
    """ë™ê¸°í™” ì‘ì—… íƒ€ì…"""
    CREATE = "create"
    UPDATE = "update"
    DELETE = "delete"
    MERGE = "merge"
    BATCH_UPDATE = "batch_update"

class DataConflictResolution(Enum):
    """ë°ì´í„° ì¶©ëŒ í•´ê²° ë°©ë²•"""
    LAST_WRITE_WINS = "last_write_wins"
    MERGE_STRATEGY = "merge_strategy"
    MANUAL_RESOLUTION = "manual_resolution"
    ROLLBACK = "rollback"

@dataclass
class SyncOperation:
    """ë™ê¸°í™” ì‘ì—… ê°ì²´"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    operation_type: SyncOperationType = SyncOperationType.CREATE
    source_system: str = ""  # 'redis', 'neo4j', 'bigquery'
    target_systems: List[str] = field(default_factory=list)
    data: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    status: str = "pending"  # pending, processing, completed, failed, conflicted
    retry_count: int = 0
    max_retries: int = 3
    error_message: Optional[str] = None
    conflict_resolution: Optional[DataConflictResolution] = None
    
    def __post_init__(self):
        if not self.target_systems:
            # ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ì‹œìŠ¤í…œì— ë™ê¸°í™”
            self.target_systems = ['redis', 'neo4j', 'bigquery']

@dataclass
class DataVersion:
    """ë°ì´í„° ë²„ì „ ì •ë³´"""
    version_id: str
    data_hash: str
    timestamp: datetime
    system: str
    operation_id: str

class EnhancedDataSyncManager:
    """í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì €"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # ì‹œìŠ¤í…œë³„ í´ë¼ì´ì–¸íŠ¸
        self.redis_client = None
        self.neo4j_driver = None
        self.bigquery_client = None
        
        # ë™ê¸°í™” í
        self.sync_queue: asyncio.Queue = asyncio.Queue()
        self.priority_queue: deque = deque()
        
        # ë™ê¸°í™” ìƒíƒœ ì¶”ì 
        self.sync_status: Dict[str, Dict[str, Any]] = defaultdict(dict)
        self.data_versions: Dict[str, List[DataVersion]] = defaultdict(list)
        
        # ì¶©ëŒ í•´ê²° ì „ëµ
        self.conflict_resolution_strategy = config.get(
            'conflict_resolution', DataConflictResolution.LAST_WRITE_WINS
        )
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        self.event_handlers: Dict[str, List[Callable]] = {
            'sync_started': [],
            'sync_completed': [],
            'sync_failed': [],
            'conflict_detected': [],
            'conflict_resolved': [],
            'rollback_required': []
        }
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = {
            'total_operations': 0,
            'successful_syncs': 0,
            'failed_syncs': 0,
            'conflicts_detected': 0,
            'conflicts_resolved': 0,
            'average_sync_time': 0.0,
            'total_sync_time': 0.0
        }
        
        # ì‹¤í–‰ ìƒíƒœ
        self.is_running = False
        self.sync_tasks: List[asyncio.Task] = []
        
        # ì¶©ëŒ í•´ê²° í
        self.conflict_queue: asyncio.Queue = asyncio.Queue()
        
        logger.info("í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™”ë¨")
    
    async def start(self):
        """ë™ê¸°í™” ë§¤ë‹ˆì € ì‹œì‘"""
        if self.is_running:
            return
        
        self.is_running = True
        
        # ë™ê¸°í™” ì‘ì—… ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹œì‘
        for _ in range(self.config.get('max_concurrent_syncs', 3)):
            self.sync_tasks.append(
                asyncio.create_task(self._sync_worker())
            )
        
        # ì¶©ëŒ í•´ê²° íƒœìŠ¤í¬ ì‹œì‘
        self.sync_tasks.append(
            asyncio.create_task(self._conflict_resolver())
        )
        
        # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì‹œì‘
        self.sync_tasks.append(
            asyncio.create_task(self._monitor_sync_health())
        )
        
        logger.info("âœ… í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € ì‹œì‘ë¨")
    
    async def stop(self):
        """ë™ê¸°í™” ë§¤ë‹ˆì € ì¤‘ì§€"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        # ëª¨ë“  íƒœìŠ¤í¬ ì·¨ì†Œ
        for task in self.sync_tasks:
            task.cancel()
        
        # íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
        await asyncio.gather(*self.sync_tasks, return_exceptions=True)
        self.sync_tasks.clear()
        
        logger.info("ğŸ›‘ í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € ì¤‘ì§€ë¨")
    
    async def sync_data(self, operation: SyncOperation) -> str:
        """ë°ì´í„° ë™ê¸°í™” ìš”ì²­"""
        # ìš°ì„ ìˆœìœ„ ê²°ì •
        priority = self._calculate_priority(operation)
        
        if priority == 'high':
            self.priority_queue.appendleft(operation)
        else:
            await self.sync_queue.put(operation)
        
        self.metrics['total_operations'] += 1
        
        logger.info(f"ğŸ“¤ ë™ê¸°í™” ìš”ì²­: {operation.operation_type.value} - {operation.source_system}")
        return operation.id
    
    async def sync_batch(self, operations: List[SyncOperation]) -> List[str]:
        """ë°°ì¹˜ ë™ê¸°í™” ìš”ì²­"""
        operation_ids = []
        
        for operation in operations:
            operation_id = await self.sync_data(operation)
            operation_ids.append(operation_id)
        
        logger.info(f"ğŸ“¦ ë°°ì¹˜ ë™ê¸°í™” ìš”ì²­: {len(operations)}ê°œ ì‘ì—…")
        return operation_ids
    
    def _calculate_priority(self, operation: SyncOperation) -> str:
        """ë™ê¸°í™” ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        # URGENT ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë†’ì€ ìš°ì„ ìˆœìœ„
        if operation.metadata.get('urgent', False):
            return 'high'
        
        # DELETE ì‘ì—…ì€ ë†’ì€ ìš°ì„ ìˆœìœ„
        if operation.operation_type == SyncOperationType.DELETE:
            return 'high'
        
        # UPDATE ì‘ì—…ì€ ì¤‘ê°„ ìš°ì„ ìˆœìœ„
        if operation.operation_type == SyncOperationType.UPDATE:
            return 'medium'
        
        # CREATE ì‘ì—…ì€ ë‚®ì€ ìš°ì„ ìˆœìœ„
        return 'low'
    
    async def _sync_worker(self):
        """ë™ê¸°í™” ì‘ì—… ì²˜ë¦¬ ì›Œì»¤"""
        while self.is_running:
            try:
                # ìš°ì„ ìˆœìœ„ íì—ì„œ ë¨¼ì € ì²˜ë¦¬
                if self.priority_queue:
                    operation = self.priority_queue.popleft()
                else:
                    # ì¼ë°˜ íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
                    operation = await asyncio.wait_for(
                        self.sync_queue.get(), 
                        timeout=1.0
                    )
                
                # ë™ê¸°í™” ì‹¤í–‰
                await self._execute_sync(operation)
                
            except asyncio.TimeoutError:
                continue
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ë™ê¸°í™” ì›Œì»¤ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _execute_sync(self, operation: SyncOperation):
        """ë™ê¸°í™” ì‹¤í–‰"""
        start_time = time.time()
        operation.status = "processing"
        
        try:
            # ë°ì´í„° ë²„ì „ í™•ì¸
            data_hash = self._calculate_data_hash(operation.data)
            
            # ì¶©ëŒ ê°ì§€
            conflicts = await self._detect_conflicts(operation, data_hash)
            
            if conflicts:
                # ì¶©ëŒì´ ê°ì§€ëœ ê²½ìš°
                await self._handle_conflicts(operation, conflicts)
                return
            
            # ë™ê¸°í™” ì‹¤í–‰
            sync_results = await self._perform_sync(operation, data_hash)
            
            # ì„±ê³µ ì²˜ë¦¬
            operation.status = "completed"
            self._update_sync_metrics(start_time, True)
            
            # ì´ë²¤íŠ¸ ë°œìƒ
            await self._trigger_event('sync_completed', {
                'operation_id': operation.id,
                'results': sync_results,
                'execution_time': time.time() - start_time
            })
            
            logger.info(f"âœ… ë™ê¸°í™” ì™„ë£Œ: {operation.id}")
            
        except Exception as e:
            # ì‹¤íŒ¨ ì²˜ë¦¬
            operation.status = "failed"
            operation.error_message = str(e)
            self._update_sync_metrics(start_time, False)
            
            # ì¬ì‹œë„ ë¡œì§
            if operation.retry_count < operation.max_retries:
                await self._schedule_retry(operation)
            else:
                # ìµœì¢… ì‹¤íŒ¨
                await self._trigger_event('sync_failed', {
                    'operation_id': operation.id,
                    'error': str(e),
                    'retry_count': operation.retry_count
                })
                
                logger.error(f"âŒ ë™ê¸°í™” ìµœì¢… ì‹¤íŒ¨: {operation.id} - {e}")
    
    async def _detect_conflicts(self, operation: SyncOperation, data_hash: str) -> List[Dict[str, Any]]:
        """ë°ì´í„° ì¶©ëŒ ê°ì§€"""
        conflicts = []
        
        for target_system in operation.target_systems:
            try:
                # ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
                existing_data = await self._get_existing_data(target_system, operation)
                
                if existing_data:
                    # ë°ì´í„° í•´ì‹œ ë¹„êµ
                    existing_hash = self._calculate_data_hash(existing_data)
                    
                    if existing_hash != data_hash:
                        # ì¶©ëŒ ê°ì§€
                        conflict = {
                            'system': target_system,
                            'existing_data': existing_data,
                            'existing_hash': existing_hash,
                            'new_data': operation.data,
                            'new_hash': data_hash,
                            'timestamp': datetime.now()
                        }
                        conflicts.append(conflict)
                        
                        logger.warning(f"âš ï¸ ì¶©ëŒ ê°ì§€ë¨: {target_system} - {operation.id}")
                
            except Exception as e:
                logger.error(f"ì¶©ëŒ ê°ì§€ ì˜¤ë¥˜ ({target_system}): {e}")
        
        if conflicts:
            self.metrics['conflicts_detected'] += 1
        
        return conflicts
    
    async def _handle_conflicts(self, operation: SyncOperation, conflicts: List[Dict[str, Any]]):
        """ì¶©ëŒ ì²˜ë¦¬"""
        operation.status = "conflicted"
        
        # ì¶©ëŒ í•´ê²° ì „ëµì— ë”°ë¥¸ ì²˜ë¦¬
        if self.conflict_resolution_strategy == DataConflictResolution.LAST_WRITE_WINS:
            # ë§ˆì§€ë§‰ ì“°ê¸° ìš°ì„ 
            await self._resolve_conflict_last_write_wins(operation, conflicts)
            
        elif self.conflict_resolution_strategy == DataConflictResolution.MERGE_STRATEGY:
            # ë³‘í•© ì „ëµ
            await self._resolve_conflict_merge(operation, conflicts)
            
        elif self.conflict_resolution_strategy == DataConflictResolution.ROLLBACK:
            # ë¡¤ë°±
            await self._resolve_conflict_rollback(operation, conflicts)
            
        else:
            # ìˆ˜ë™ í•´ê²° ëŒ€ê¸°
            await self.conflict_queue.put({
                'operation': operation,
                'conflicts': conflicts,
                'timestamp': datetime.now()
            })
        
        # ì´ë²¤íŠ¸ ë°œìƒ
        await self._trigger_event('conflict_detected', {
            'operation_id': operation.id,
            'conflicts': conflicts,
            'resolution_strategy': self.conflict_resolution_strategy.value
        })
    
    async def _resolve_conflict_last_write_wins(self, operation: SyncOperation, conflicts: List[Dict[str, Any]]):
        """ë§ˆì§€ë§‰ ì“°ê¸° ìš°ì„  ì¶©ëŒ í•´ê²°"""
        logger.info(f"ğŸ”„ ë§ˆì§€ë§‰ ì“°ê¸° ìš°ì„ ìœ¼ë¡œ ì¶©ëŒ í•´ê²°: {operation.id}")
        
        # ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ë®ì–´ì“°ê¸°
        operation.conflict_resolution = DataConflictResolution.LAST_WRITE_WINS
        
        # ë™ê¸°í™” ì¬ì‹œë„
        await self.sync_data(operation)
    
    async def _resolve_conflict_merge(self, operation: SyncOperation, conflicts: List[Dict[str, Any]]):
        """ë³‘í•© ì „ëµ ì¶©ëŒ í•´ê²°"""
        logger.info(f"ğŸ”„ ë³‘í•© ì „ëµìœ¼ë¡œ ì¶©ëŒ í•´ê²°: {operation.id}")
        
        # ë°ì´í„° ë³‘í•©
        merged_data = operation.data.copy()
        
        for conflict in conflicts:
            existing_data = conflict['existing_data']
            # ê°„ë‹¨í•œ ë³‘í•© ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
            for key, value in existing_data.items():
                if key not in merged_data:
                    merged_data[key] = value
        
        # ë³‘í•©ëœ ë°ì´í„°ë¡œ ë™ê¸°í™”
        operation.data = merged_data
        operation.conflict_resolution = DataConflictResolution.MERGE_STRATEGY
        
        # ë™ê¸°í™” ì¬ì‹œë„
        await self.sync_data(operation)
    
    async def _resolve_conflict_rollback(self, operation: SyncOperation, conflicts: List[Dict[str, Any]]):
        """ë¡¤ë°± ì¶©ëŒ í•´ê²°"""
        logger.info(f"ğŸ”„ ë¡¤ë°±ìœ¼ë¡œ ì¶©ëŒ í•´ê²°: {operation.id}")
        
        operation.conflict_resolution = DataConflictResolution.ROLLBACK
        operation.status = "cancelled"
        
        # ì´ë²¤íŠ¸ ë°œìƒ
        await self._trigger_event('rollback_required', {
            'operation_id': operation.id,
            'conflicts': conflicts
        })
    
    async def _perform_sync(self, operation: SyncOperation, data_hash: str) -> Dict[str, Any]:
        """ì‹¤ì œ ë™ê¸°í™” ìˆ˜í–‰"""
        sync_results = {}
        
        for target_system in operation.target_systems:
            try:
                if target_system == 'redis':
                    result = await self._sync_to_redis(operation, data_hash)
                elif target_system == 'neo4j':
                    result = await self._sync_to_neo4j(operation, data_hash)
                elif target_system == 'bigquery':
                    result = await self._sync_to_bigquery(operation, data_hash)
                else:
                    result = {'success': False, 'error': f'Unknown system: {target_system}'}
                
                sync_results[target_system] = result
                
                # ì„±ê³µí•œ ê²½ìš° ë°ì´í„° ë²„ì „ ì—…ë°ì´íŠ¸
                if result.get('success', False):
                    await self._update_data_version(operation, target_system, data_hash)
                
            except Exception as e:
                sync_results[target_system] = {
                    'success': False,
                    'error': str(e)
                }
                logger.error(f"ë™ê¸°í™” ì‹¤íŒ¨ ({target_system}): {e}")
        
        return sync_results
    
    async def _sync_to_redis(self, operation: SyncOperation, data_hash: str) -> Dict[str, Any]:
        """Redis ë™ê¸°í™”"""
        try:
            if not self.redis_client:
                return {'success': False, 'error': 'Redis client not available'}
            
            key = operation.metadata.get('redis_key', f"argo:{operation.id}")
            
            if operation.operation_type == SyncOperationType.DELETE:
                await self.redis_client.delete(key)
            else:
                # CREATE, UPDATE, MERGE
                await self.redis_client.set(key, json.dumps(operation.data))
                await self.redis_client.expire(key, 3600)  # 1ì‹œê°„ TTL
            
            return {'success': True, 'key': key}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _sync_to_neo4j(self, operation: SyncOperation, data_hash: str) -> Dict[str, Any]:
        """Neo4j ë™ê¸°í™”"""
        try:
            if not self.neo4j_driver:
                return {'success': False, 'error': 'Neo4j driver not available'}
            
            # ê°„ë‹¨í•œ Cypher ì¿¼ë¦¬ ìƒì„±
            if operation.operation_type == SyncOperationType.DELETE:
                query = "MATCH (n {id: $id}) DELETE n"
                params = {'id': operation.id}
            else:
                # CREATE, UPDATE, MERGE
                query = """
                MERGE (n {id: $id})
                SET n += $properties
                RETURN n
                """
                params = {
                    'id': operation.id,
                    'properties': operation.data
                }
            
            # ì¿¼ë¦¬ ì‹¤í–‰
            with self.neo4j_driver.session() as session:
                result = session.run(query, params)
                record = result.single()
            
            return {'success': True, 'record': record.data() if record else None}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _sync_to_bigquery(self, operation: SyncOperation, data_hash: str) -> Dict[str, Any]:
        """BigQuery ë™ê¸°í™”"""
        try:
            if not self.bigquery_client:
                return {'success': False, 'error': 'BigQuery client not available'}
            
            # BigQuery í…Œì´ë¸” ì´ë¦„
            table_name = operation.metadata.get('bigquery_table', 'argo_sync_log')
            
            # ë°ì´í„° ì¤€ë¹„
            row_data = {
                'operation_id': operation.id,
                'operation_type': operation.operation_type.value,
                'source_system': operation.source_system,
                'data_hash': data_hash,
                'timestamp': operation.timestamp.isoformat(),
                'data': json.dumps(operation.data)
            }
            
            # ë°ì´í„° ì‚½ì… (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë¡œì§ í•„ìš”)
            # self.bigquery_client.insert_rows_json(table_name, [row_data])
            
            return {'success': True, 'table': table_name}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _get_existing_data(self, system: str, operation: SyncOperation) -> Optional[Dict[str, Any]]:
        """ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ"""
        try:
            if system == 'redis':
                if self.redis_client:
                    key = operation.metadata.get('redis_key', f"argo:{operation.id}")
                    data = await self.redis_client.get(key)
                    return json.loads(data) if data else None
            
            elif system == 'neo4j':
                if self.neo4j_driver:
                    query = "MATCH (n {id: $id}) RETURN n"
                    with self.neo4j_driver.session() as session:
                        result = session.run(query, {'id': operation.id})
                        record = result.single()
                        return record.data() if record else None
            
            return None
            
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜ ({system}): {e}")
            return None
    
    def _calculate_data_hash(self, data: Dict[str, Any]) -> str:
        """ë°ì´í„° í•´ì‹œ ê³„ì‚°"""
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(data_str.encode()).hexdigest()
    
    async def _update_data_version(self, operation: SyncOperation, system: str, data_hash: str):
        """ë°ì´í„° ë²„ì „ ì—…ë°ì´íŠ¸"""
        version = DataVersion(
            version_id=str(uuid.uuid4()),
            data_hash=data_hash,
            timestamp=datetime.now(),
            system=system,
            operation_id=operation.id
        )
        
        self.data_versions[operation.id].append(version)
    
    async def _schedule_retry(self, operation: SyncOperation):
        """ì¬ì‹œë„ ìŠ¤ì¼€ì¤„ë§"""
        operation.retry_count += 1
        
        # ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„ ì§€ì—°
        delay = 2 ** operation.retry_count
        
        asyncio.create_task(self._delayed_retry(operation, delay))
        
        logger.info(f"ğŸ”„ ì¬ì‹œë„ ì˜ˆì•½: {operation.id} ({operation.retry_count}/{operation.max_retries}) - {delay}ì´ˆ í›„")
    
    async def _delayed_retry(self, operation: SyncOperation, delay: int):
        """ì§€ì—°ëœ ì¬ì‹œë„"""
        await asyncio.sleep(delay)
        
        if self.is_running:
            operation.status = "pending"
            await self.sync_data(operation)
    
    async def _conflict_resolver(self):
        """ì¶©ëŒ í•´ê²° ì²˜ë¦¬ê¸°"""
        while self.is_running:
            try:
                # ì¶©ëŒ íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
                conflict_data = await asyncio.wait_for(
                    self.conflict_queue.get(), 
                    timeout=1.0
                )
                
                # ìˆ˜ë™ í•´ê²° ëŒ€ê¸° (ì‹¤ì œë¡œëŠ” ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ í•„ìš”)
                logger.info(f"â³ ì¶©ëŒ ìˆ˜ë™ í•´ê²° ëŒ€ê¸°: {conflict_data['operation'].id}")
                
            except asyncio.TimeoutError:
                continue
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ì¶©ëŒ í•´ê²° ì²˜ë¦¬ê¸° ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _monitor_sync_health(self):
        """ë™ê¸°í™” ìƒíƒœ ëª¨ë‹ˆí„°ë§"""
        while self.is_running:
            try:
                # í ìƒíƒœ í™•ì¸
                queue_size = self.sync_queue.qsize()
                priority_size = len(self.priority_queue)
                conflict_size = self.conflict_queue.qsize()
                
                # ê²½ê³  ì¡°ê±´ í™•ì¸
                if queue_size > 100:
                    logger.warning(f"âš ï¸ ë™ê¸°í™” í í¬ê¸° ì¦ê°€: {queue_size}")
                
                if conflict_size > 10:
                    logger.warning(f"âš ï¸ ì¶©ëŒ í í¬ê¸° ì¦ê°€: {conflict_size}")
                
                # ë©”íŠ¸ë¦­ ë¡œê¹…
                logger.debug(f"ë™ê¸°í™” ìƒíƒœ - ì¼ë°˜: {queue_size}, ìš°ì„ ìˆœìœ„: {priority_size}, ì¶©ëŒ: {conflict_size}")
                
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ë™ê¸°í™” ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(5)
    
    def _update_sync_metrics(self, start_time: float, success: bool):
        """ë™ê¸°í™” ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        execution_time = time.time() - start_time
        
        if success:
            self.metrics['successful_syncs'] += 1
        else:
            self.metrics['failed_syncs'] += 1
        
        # í‰ê·  ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_successful = self.metrics['successful_syncs']
        if total_successful > 0:
            current_avg = self.metrics['average_sync_time']
            self.metrics['average_sync_time'] = (
                (current_avg * (total_successful - 1) + execution_time) / total_successful
            )
        
        self.metrics['total_sync_time'] += execution_time
    
    async def _trigger_event(self, event_type: str, data: Dict[str, Any]):
        """ì´ë²¤íŠ¸ ë°œìƒ"""
        if event_type in self.event_handlers:
            for handler in self.event_handlers[event_type]:
                try:
                    await handler(data)
                except Exception as e:
                    logger.error(f"ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì˜¤ë¥˜: {e}")
    
    def add_event_handler(self, event_type: str, handler: Callable):
        """ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì¶”ê°€"""
        if event_type in self.event_handlers:
            self.event_handlers[event_type].append(handler)
    
    async def get_sync_status(self) -> Dict[str, Any]:
        """ë™ê¸°í™” ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'is_running': self.is_running,
            'queue_sizes': {
                'sync_queue': self.sync_queue.qsize(),
                'priority_queue': len(self.priority_queue),
                'conflict_queue': self.conflict_queue.qsize()
            },
            'metrics': self.metrics.copy(),
            'config': {
                'conflict_resolution': self.conflict_resolution_strategy.value,
                'max_concurrent_syncs': self.config.get('max_concurrent_syncs', 3)
            }
        }

# ì‚¬ìš© ì˜ˆì‹œ
async def example_sync_handler(data: Dict[str, Any]):
    """ì˜ˆì‹œ ë™ê¸°í™” ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    print(f"ğŸ”„ ë™ê¸°í™” ì´ë²¤íŠ¸: {data}")

async def main():
    """í–¥ìƒëœ ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸"""
    config = {
        'max_concurrent_syncs': 3,
        'conflict_resolution': DataConflictResolution.LAST_WRITE_WINS
    }
    
    # ë™ê¸°í™” ë§¤ë‹ˆì € ìƒì„±
    sync_manager = EnhancedDataSyncManager(config)
    
    # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡
    sync_manager.add_event_handler('sync_completed', example_sync_handler)
    sync_manager.add_event_handler('conflict_detected', example_sync_handler)
    
    try:
        # ë§¤ë‹ˆì € ì‹œì‘
        await sync_manager.start()
        
        # í…ŒìŠ¤íŠ¸ ë™ê¸°í™” ì‘ì—… ìƒì„±
        operations = [
            SyncOperation(
                operation_type=SyncOperationType.CREATE,
                source_system='redis',
                data={'name': 'Test Data 1', 'value': 100},
                metadata={'redis_key': 'test:data:1'}
            ),
            SyncOperation(
                operation_type=SyncOperationType.UPDATE,
                source_system='neo4j',
                data={'name': 'Test Data 2', 'value': 200},
                metadata={'redis_key': 'test:data:2'}
            )
        ]
        
        # ë™ê¸°í™” ìš”ì²­
        operation_ids = await sync_manager.sync_batch(operations)
        print(f"ë™ê¸°í™” ìš”ì²­ë¨: {operation_ids}")
        
        # ìƒíƒœ ëª¨ë‹ˆí„°ë§
        for _ in range(10):
            status = await sync_manager.get_sync_status()
            print(f"ë™ê¸°í™” ìƒíƒœ: {status['queue_sizes']}")
            await asyncio.sleep(2)
        
    finally:
        await sync_manager.stop()

if __name__ == "__main__":
    asyncio.run(main())
